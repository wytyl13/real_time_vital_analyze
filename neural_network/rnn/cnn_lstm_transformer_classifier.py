#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time Â  Â : 2025/07/29 09:48
@Author Â : weiyutao
@File Â  Â : cnn_lstm_transformer_classifier.py

1ã€SimpleSleepDatasetä¸­åˆå§‹åŒ–çš„step_sizeæ˜¯5è¿˜æ˜¯30
2ã€æŒ‰ç…§step_sizeä½œä¸ºæ»‘åŠ¨æ­¥ä¼ï¼Œ
"""


import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import json
import pickle  # ç”¨äºä¿å­˜scaler
import os  # ç”¨äºæ–‡ä»¶è·¯å¾„æ£€æŸ¥
import random
import matplotlib.animation as animation
from collections import deque
import threading
import time
import pickle  # ç”¨äºä¿å­˜è®­ç»ƒå†å²


# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'WenQuanYi Micro Hei']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜


class SimpleSleepDataset(Dataset):
    """ç®€å•çš„ç¡çœ æ•°æ®é›† - 3ä¸ªç¡çœ é˜¶æ®µ"""
    
    def __init__(self, csv_file, window_size=60, step_size=30, max_samples=None, scaler_path=None):
        # è¯»å–æ•°æ®
        print("æ­£åœ¨è¯»å–CSVæ–‡ä»¶...")
        self.df = pd.read_csv(csv_file, encoding='utf-8')
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œåˆ™åªä½¿ç”¨éƒ¨åˆ†æ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        if max_samples and len(self.df) > max_samples:
            print(f"ä½¿ç”¨å‰ {max_samples} è¡Œæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•...")
            self.df = self.df.head(max_samples)
        
        self.window_size = window_size
        
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")
        print(f"åˆ—å: {self.df.columns.tolist()}")
        
        # å¤„ç†ä¸­æ–‡æ ‡ç­¾ - ä½ çš„3ä¸ªæ ‡ç­¾
        label_mapping = {
            'æ¸…é†’': 0,    # Wake
            'æµ…ç¡çœ ': 1,  # Light Sleep 
            'æ·±ç¡çœ ': 2,  # Deep Sleep
            # å¦‚æœæœ‰å…¶ä»–å˜ä½“å†™æ³•ï¼Œä¹Ÿå¯ä»¥æ˜ å°„
            'æ¸…é†’çŠ¶æ€': 0,
            'æµ…åº¦ç¡çœ ': 1,
            'æ·±åº¦ç¡çœ ': 2,
        }
        
        
        # æ ¹æ®ä½ çš„æ•°æ®ï¼Œlabelåˆ—åŒ…å«ä¸­æ–‡
        if 'label' in self.df.columns:
            # æ˜ å°„ä¸­æ–‡æ ‡ç­¾ä¸ºæ•°å­—
            self.df['label_num'] = self.df['label'].map(label_mapping)
            # å¦‚æœæœ‰æœªæ˜ å°„çš„æ ‡ç­¾ï¼Œè®¾ä¸º0ï¼ˆæ¸…é†’ï¼‰
            self.df['label_num'] = self.df['label_num'].fillna(0).astype(int)
            print(f"æ ‡ç­¾æ˜ å°„: {self.df['label'].value_counts()}")
        else:
            self.df['label_num'] = 0

        
        # ä½¿ç”¨ä½ çš„å®é™…åˆ—åï¼šbreath_line, heart_line
        # æ·»åŠ é¢å¤–ç‰¹å¾æå‡æ•ˆæœ
        print("æ­£åœ¨è®¡ç®—ç‰¹å¾å·¥ç¨‹...")
        self.df['heart_rate'] = self.df['heart_line'].rolling(10, min_periods=1).std() * 100 + 70
        self.df['resp_rate'] = self.df['breath_line'].rolling(10, min_periods=1).std() * 50 + 15
        print("ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        
        # 5ä¸ªç‰¹å¾ï¼šå‘¼å¸ç‡ã€å¿ƒç‡ã€å‘¼å¸çº¿ã€å¿ƒçº¿ã€ä¿¡å·è´¨é‡
        # features = ['resp_rate', 'heart_rate', 'breath_line', 'heart_line', 'signal_intensity']
        # features = ['breath_bpm', 'heart_bpm', 'breath_line', 'heart_line', 'distance', 'signal_intensity', 'state']
        features = ['breath_bpm', 'heart_bpm', 'breath_line', 'heart_line', 'distance']
        # features = ['breath_line', 'heart_line']
        
        # ä¿®å¤ï¼šå¤„ç†ç¼ºå¤±å€¼ - ä½¿ç”¨æ–°çš„pandasè¯­æ³•
        print("å¤„ç†ç¼ºå¤±å€¼...")
        self.df[features] = self.df[features].ffill().fillna(0)
        
        # å…³é”®ä¿®å¤ï¼šæ ‡å‡†åŒ–å¤„ç†
        if scaler_path and os.path.exists(scaler_path):
            print(f"sleep_scaler.pkl: ------------------- {scaler_path}")
            # é¢„æµ‹æ—¶ï¼šåŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„scaler
            print(f"åŠ è½½é¢„è®­ç»ƒçš„scaler: {scaler_path}")
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            self.df[features] = scaler.transform(self.df[features])
            print("ä½¿ç”¨é¢„è®­ç»ƒscalerå®Œæˆæ•°æ®æ ‡å‡†åŒ–!")
        else:
            # è®­ç»ƒæ—¶ï¼šåˆ›å»ºæ–°çš„scalerå¹¶ä¿å­˜
            print("åˆ›å»ºæ–°çš„scalerè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
            scaler = StandardScaler()
            self.df[features] = scaler.fit_transform(self.df[features])
            # ä¿å­˜scalerä¾›é¢„æµ‹æ—¶ä½¿ç”¨
            scaler_save_path = scaler_path if scaler_path else 'sleep_scaler.pkl'
            with open(scaler_save_path, 'wb') as f:
                pickle.dump(scaler, f)
            print(f"æ•°æ®æ ‡å‡†åŒ–å®Œæˆ! Scalerå·²ä¿å­˜åˆ°: {scaler_save_path}")
        
        # æ‰“å°æ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡
        print("æ ‡å‡†åŒ–åçš„ç‰¹å¾ç»Ÿè®¡:")
        for feature in features:
            mean_val = self.df[feature].mean()
            std_val = self.df[feature].std()
            print(f"  {feature}: å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}")
        
        # åˆ›å»ºçª—å£ - æ·»åŠ è¿›åº¦æ˜¾ç¤º
        print("å¼€å§‹åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬...")
        self.windows = []
        self.labels = []
        
        total_windows = (len(self.df) - window_size) // step_size + 1
        print(f"é¢„è®¡åˆ›å»º {total_windows} ä¸ªçª—å£æ ·æœ¬...")
        
        for i in range(0, len(self.df) - window_size + 1, step_size):
            window = self.df[features].iloc[i:i+window_size].values
            label = self.df['label_num'].iloc[i+step_size-1]
            
            self.windows.append(window)
            self.labels.append(label)
            
            # æ¯1000ä¸ªçª—å£æ‰“å°ä¸€æ¬¡è¿›åº¦
            if len(self.windows) % 1000 == 0:
                progress = len(self.windows) / total_windows * 100
                print(f"è¿›åº¦: {len(self.windows)}/{total_windows} ({progress:.1f}%)")
        
        self.windows = np.array(self.windows)
        self.labels = np.array(self.labels)
        
        print(f"åˆ›å»ºäº† {len(self.windows)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®å½¢çŠ¶: {self.windows.shape}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.labels)}")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒè¯¦æƒ…
        unique_labels, counts = np.unique(self.labels, return_counts=True)
        label_names = ['æ¸…é†’', 'æµ…ç¡çœ ', 'æ·±ç¡çœ ']  # 3ä¸ªæ ‡ç­¾
        for label, count in zip(unique_labels, counts):
            label_name = label_names[label] if label < len(label_names) else f'æ ‡ç­¾{label}'
            print(f"  {label_name}: {count}ä¸ªæ ·æœ¬ ({count/len(self.labels)*100:.1f}%)")
    
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, idx):
        # ä¿®å¤ï¼šç›´æ¥è¿”å›æ ‡é‡æ ‡ç­¾ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.labels[idx]]).squeeze()

class SimpleSleepNet(nn.Module):
    """è¶…ç®€å•çš„ç¡çœ åˆ†æœŸç½‘ç»œ - 3ä¸ªç¡çœ é˜¶æ®µ"""
    
    def __init__(
        self, 
        input_size=5, 
        seq_length=30, 
        num_classes=3, 
        bidirectional_flag=False, 
        lstm_hidden_size=64, 
        transformer_flag=False, 
        transformer_num_heads=2,
        dropout=0.3
    ):  # 3ä¸ªç±»åˆ«ï¼šæ¸…é†’ã€æµ…ç¡çœ ã€æ·±ç¡çœ 
        super().__init__()
        
        # ç®€å•çš„CNN+LSTM
        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool1d(1)
        
        # åŒå‘LSTMæå‡æ•ˆæœ - ä¿®å¤dropoutè­¦å‘Š
        self.lstm = nn.LSTM(128, lstm_hidden_size, num_layers=2, batch_first=True, dropout=dropout, bidirectional=bidirectional_flag)
        classifier_input_size = lstm_hidden_size * 2 if bidirectional_flag else lstm_hidden_size
        self.transformer_flag = transformer_flag
        self.pos_encoder = PositionalEncoding(classifier_input_size, max_len=seq_length)
        self.transformer = nn.ModuleList([
            TransformerBlock(classifier_input_size, num_heads=transformer_num_heads, dropout=dropout)
        ])
        print(f"classifier_input_size: -------------------------------- {classifier_input_size}")
        self.classifier = nn.Sequential(
            nn.Linear(classifier_input_size, classifier_input_size // 2),  # åŒå‘LSTMè¾“å‡º128ç»´
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(classifier_input_size // 2, classifier_input_size // 4),
            nn.ReLU(), 
            nn.Dropout(dropout),
            nn.Linear(classifier_input_size // 4, num_classes)  # 3ä¸ªè¾“å‡º
        )

        
        
    def forward(self, x):
        x = x.transpose(1, 2)
        
        # æ›´æ·±çš„CNNæå–ç‰¹å¾
        # x dimension (batch_size, seq_length, feature_size) = (128, 60, 5)
        # kernel_size (32, 3, 5) = 480
        # output_dimension (128, 32, 60)
        # kernel_size (64, 3, 5) = 960
        # output_dimension (128, 64, 60)
        # kernel_size (128, 3, 5) = 1920
        # output_dimension (128, 128, 60)

        x = torch.relu(self.conv1(x)) #  dimension (batch_size, 32, 60) 
        x = torch.relu(self.conv2(x)) # dimension (128, 64, 60)
        x = torch.relu(self.conv3(x)) # dimension (128, 128, 60)
        x = x.transpose(1, 2) # (128, 60, 128)
        
        # num = 2 f bidirections else 1
        lstm_out, _ = self.lstm(x) # (128, 60, lstm_hidden_size * num)

        # transformer
        if self.transformer_flag:
            transformer_in = self.pos_encoder(lstm_out) # (128, 60, lstm_hidden_size * num)
            for layer in self.transformer:
                transformer_in = layer(transformer_in)
            # transformer_in dimension (128, 60, 128)
            transformer_out = torch.mean(transformer_in, dim=1) # dimension (128, 128)
            lstm_out = transformer_out
        else:
            lstm_out = torch.mean(lstm_out, dim=1)
            print(f"lstm_out.shape: --------------------------------- {lstm_out.shape}")
        # åˆ†ç±»
        output = self.classifier(lstm_out)
        return output

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç  - ä¸ºTransformeræä¾›åºåˆ—ä½ç½®ä¿¡æ¯"""
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [0, 1, 2, ...]
        # PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
        # e^([0, 2, 4, 6, ...] * -ln(10000 / 128))
        # 1/e^(2i * ln(10000)/128))
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # x: [batch_size, seq_len, embedding_dim]
        return x + self.pe[:x.size(1), :]


class TransformerBlock(nn.Module):
    """Transformeræ¨¡å— - åŒ…å«è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œ"""
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.dropout1 = nn.Dropout(dropout)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(4 * embed_dim, embed_dim)
        )
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout1(attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout2(ff_output))
        
        return x


class LSTMTransformerSleepNet(nn.Module):
    """LSTM+Transformeræ··åˆæ¨¡å‹ - ç”¨äºç¡çœ åˆ†æœŸ"""
    
    def __init__(self, input_size=5, seq_length=60, num_classes=3, 
                 lstm_hidden_size=64, num_transformer_layers=2, num_heads=4, dropout=0.3):
        super().__init__()
        
        # 1. CNNéƒ¨åˆ† - æå–å±€éƒ¨ç‰¹å¾
        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        
        # 2. LSTMéƒ¨åˆ† - å¤„ç†æ—¶åºåŠ¨æ€
        self.lstm = nn.LSTM(128, lstm_hidden_size, num_layers=2, batch_first=True, 
                           dropout=dropout, bidirectional=True)
        
        # 3. Transformeréƒ¨åˆ† - æ•è·é•¿è·ç¦»ä¾èµ–
        transformer_dim = lstm_hidden_size * 2  # åŒå‘LSTMè¾“å‡ºç»´åº¦
        self.pos_encoder = PositionalEncoding(transformer_dim, max_len=seq_length)
        self.transformer_layers = nn.ModuleList([
            TransformerBlock(transformer_dim, num_heads, dropout)
            for _ in range(num_transformer_layers)
        ])
        
        # 4. åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(transformer_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout/2),
            nn.Linear(32, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x: [batch, seq_len, features] -> [batch, features, seq_len]
        batch_size = x.size(0)
        x_cnn = x.transpose(1, 2)
        
        # 1. CNNç‰¹å¾æå–
        cnn_out = torch.relu(self.conv1(x_cnn))
        cnn_out = torch.relu(self.conv2(cnn_out))
        cnn_out = torch.relu(self.conv3(cnn_out))  # [batch, 128, seq_len]
        
        # 2. LSTMå¤„ç†
        # è°ƒæ•´ç»´åº¦ä»¥é€‚åº”LSTMè¾“å…¥ [batch, seq_len, features]
        lstm_in = cnn_out.transpose(1, 2)  # [batch, seq_len, 128]
        lstm_out, _ = self.lstm(lstm_in)  # [batch, seq_len, 128]
        
        # 3. Transformerå¤„ç†
        transformer_in = self.pos_encoder(lstm_out)  # æ·»åŠ ä½ç½®ç¼–ç 
        
        for layer in self.transformer_layers:
            transformer_in = layer(transformer_in)
        
        # å…¨å±€æ± åŒ–è·å–åºåˆ—è¡¨ç¤º
        transformer_out = torch.mean(transformer_in, dim=1)  # [batch, 128]
        
        # 4. åˆ†ç±»
        output = self.classifier(transformer_out)
        
        return output



def train_simple_model(
    epochs=50, 
    max_samples=None, 
    patience=10, 
    min_delta=0.001, 
    param_name=None, 
    dataset=None,
    input_size=None,
    bidirectional_flag=None,
    transformer_flag=None
):
    import shutil
    """
    è®­ç»ƒå‡½æ•°
    Args:
        csv_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        epochs: æœ€å¤§è®­ç»ƒè½®æ•°
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        patience: æ—©åœè€å¿ƒå€¼ï¼Œè¿ç»­å¤šå°‘ä¸ªepochéªŒè¯æŸå¤±ä¸æ”¹å–„å°±åœæ­¢
        min_delta: è®¤ä¸ºæ˜¯æ”¹å–„çš„æœ€å°å˜åŒ–é‡
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    print("å¼€å§‹è®­ç»ƒç®€å•ç¡çœ åˆ†æœŸæ¨¡å‹...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = f"/work/ai/WHOAMI/train_data/train/{param_name}"
    # å¦‚æœç›®å½•å­˜åœ¨ï¼Œå…ˆæ¸…ç©ºå®ƒ
    if os.path.exists(save_dir):
        print(f"ç›®å½• {save_dir} å·²å­˜åœ¨ï¼Œæ­£åœ¨æ¸…ç©º...")
        shutil.rmtree(save_dir)
        print("ç›®å½•å·²æ¸…ç©º")
    os.makedirs(save_dir, exist_ok=True)
    print(f"æ¨¡å‹å’Œå›¾ç‰‡å°†ä¿å­˜åˆ°: {save_dir}")
    
    
    
    # 2. åˆ†å‰²æ•°æ®é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    generator = torch.Generator().manual_seed(42)
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=generator)
    
    # ä¿®å¤ï¼šæ·»åŠ drop_last=Trueé¿å…ä¸å®Œæ•´çš„batchå¯¼è‡´é”™è¯¯
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True, generator=torch.Generator().manual_seed(42))
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True, generator=torch.Generator().manual_seed(42))
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}, éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    
    
    # 3. åˆ›å»ºæ¨¡å‹ - 3ä¸ªç¡çœ é˜¶æ®µ
    model = SimpleSleepNet(input_size=input_size, seq_length=60, num_classes=3, bidirectional_flag=bidirectional_flag, lstm_hidden_size=64, transformer_flag=transformer_flag).to(device)
    
    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - ä¸ºå°‘æ•°ç±»åˆ«åŠ æƒ
    class_counts = np.bincount(dataset.labels)
    class_weights = 1.0 / (class_counts + 1e-6)  # é¿å…é™¤é›¶
    class_weights = torch.FloatTensor(class_weights / class_weights.sum() * len(class_weights))
    criterion = nn.CrossEntropyLoss()
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # æ—©åœæœºåˆ¶ç›¸å…³å˜é‡
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    best_epoch = 0
    
    print(f"æ—©åœè®¾ç½®: patience={patience}, min_delta={min_delta}")
    
    # 4. è®¾ç½®ç»˜å›¾
    plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('real time loss', fontsize=16)
    
    # æ•°æ®å­˜å‚¨
    train_losses_history = []
    val_losses_history = []
    val_accs_history = []
    batch_train_losses = deque(maxlen=100)  # åªä¿ç•™æœ€è¿‘100ä¸ªbatchçš„æŸå¤±
    epochs_completed = []
    
    # å­å›¾è®¾ç½®
    ax1 = axes[0, 0]  # æ¯ä¸ªepochçš„æŸå¤±
    ax2 = axes[0, 1]  # éªŒè¯å‡†ç¡®ç‡
    ax3 = axes[1, 0]  # å®æ—¶batchæŸå¤±
    ax4 = axes[1, 1]  # è®­ç»ƒå’ŒéªŒè¯æŸå¤±å¯¹æ¯”
    
    def update_plots():
        """æ›´æ–°æ‰€æœ‰å­å›¾"""
        # æ¸…é™¤ä¹‹å‰çš„å›¾
        for ax in axes.flat:
            ax.clear()
        
        # 1. EpochæŸå¤±æ›²çº¿
        if train_losses_history and val_losses_history:
            ax1.plot(range(1, len(train_losses_history) + 1), train_losses_history, 'b-', label='Train Loss', linewidth=2)
            ax1.plot(range(1, len(val_losses_history) + 1), val_losses_history, 'r-', label='Val Loss', linewidth=2)
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.set_title('Epoch Loss Curve')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # 2. éªŒè¯å‡†ç¡®ç‡
        if val_accs_history:
            ax2.plot(range(1, len(val_accs_history) + 1), val_accs_history, 'g-', linewidth=2, marker='o', markersize=4)
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Accuracy')
            ax2.set_title('Val Accuracy')
            ax2.grid(True, alpha=0.3)
            ax2.set_ylim(0, 1)
        
        # 3. å®æ—¶BatchæŸå¤±ï¼ˆæœ€è¿‘100ä¸ªbatchï¼‰
        if batch_train_losses:
            ax3.plot(list(batch_train_losses), 'orange', linewidth=1, alpha=0.7)
            ax3.set_xlabel('Recent Batches')
            ax3.set_ylabel('Batch Loss')
            ax3.set_title('Real Time Batch Loss (Recent 100)')
            ax3.grid(True, alpha=0.3)
        
        # 4. è®­ç»ƒä¸éªŒè¯æŸå¤±å¯¹æ¯”
        if train_losses_history and val_losses_history:
            epochs_range = range(1, len(train_losses_history) + 1)
            ax4.plot(epochs_range, train_losses_history, 'b-', label='Train', linewidth=2, marker='o', markersize=3)
            ax4.plot(epochs_range, val_losses_history, 'r-', label='Validation', linewidth=2, marker='s', markersize=3)
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Loss')
            ax4.set_title('Train VS Val Loss')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.pause(0.01)  # çŸ­æš‚æš‚åœä»¥æ›´æ–°æ˜¾ç¤º
    
    def save_checkpoint_and_plot(epoch, model, optimizer, train_loss, val_loss, val_acc, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹å’Œå¯¹åº”çš„è®­ç»ƒå›¾è¡¨"""
        # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        suffix = "_best" if is_best else f"_epoch_{epoch}"
        checkpoint_path = os.path.join(save_dir, f'checkpoint{suffix}.pth')
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'best_val_loss': best_val_loss,
            'patience_counter': patience_counter,
        }, checkpoint_path)
        
        # ä¿å­˜å¯¹åº”çš„è®­ç»ƒå›¾è¡¨
        plot_path = os.path.join(save_dir, f'training_curves{suffix}.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        
        print(f'æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}')
        print(f'è®­ç»ƒå›¾è¡¨å·²ä¿å­˜: {plot_path}')
    
    # 5. è®­ç»ƒå¾ªç¯
    try:
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            batch_count = 0
            
            print(f"\n{'='*50}")
            print(f"Epoch {epoch+1}/{epochs}")
            print(f"{'='*50}")
            
            for batch_idx, (batch_x, batch_y) in enumerate(train_loader):
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                if batch_y.dim() > 1:
                    batch_y = batch_y.squeeze()
                
                if batch_x.size(0) == 0 or batch_y.size(0) == 0:
                    continue
                
                optimizer.zero_grad()
                outputs = model(batch_x)
                # batch_y dimension (128, 3) one-hot [0, 1] 
                # outputs dimension (128, 3) 
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                current_loss = loss.item()
                train_loss += current_loss
                batch_count += 1
                
                # æ·»åŠ åˆ°batchæŸå¤±å†å²
                batch_train_losses.append(current_loss)
                
                # å®æ—¶æ‰“å°è®­ç»ƒè¿›åº¦
                progress = (batch_idx + 1) / len(train_loader) * 100
                print(f"\rEpoch {epoch+1} [{batch_idx+1:4d}/{len(train_loader):4d}] ({progress:5.1f}%) "
                      f"Batch Loss: {current_loss:.4f} | Avg Loss: {train_loss/(batch_idx+1):.4f}", end='', flush=True)
            
            avg_train_loss = train_loss / batch_count if batch_count > 0 else 0
            train_losses_history.append(avg_train_loss)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_correct = 0
            val_total = 0
            val_loss = 0
            val_batch_count = 0
            
            print(f"\n{'-'*30} éªŒè¯é˜¶æ®µ {'-'*30}")
            
            with torch.no_grad():
                for batch_idx, (batch_x, batch_y) in enumerate(val_loader):
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    if batch_y.dim() > 1:
                        batch_y = batch_y.squeeze()
                    
                    if batch_x.size(0) == 0 or batch_y.size(0) == 0:
                        continue
                    
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    val_batch_count += 1
                    
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += batch_y.size(0)
                    val_correct += (predicted == batch_y).sum().item()
                    
                    # å®æ—¶æ‰“å°éªŒè¯è¿›åº¦
                    val_progress = (batch_idx + 1) / len(val_loader) * 100
                    current_acc = val_correct / val_total if val_total > 0 else 0
                    print(f"\réªŒè¯è¿›åº¦ [{batch_idx+1:3d}/{len(val_loader):3d}] ({val_progress:5.1f}%) "
                          f"Val Loss: {loss.item():.4f} | Acc: {current_acc:.4f}", end='', flush=True)
            
            # è®¡ç®—epochç»Ÿè®¡
            val_acc = val_correct / val_total if val_total > 0 else 0
            avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else 0
            
            val_losses_history.append(avg_val_loss)
            val_accs_history.append(val_acc)
            epochs_completed.append(epoch + 1)
            
            # æ›´æ–°å›¾è¡¨
            update_plots()
            
            # æ—©åœæ£€æŸ¥
            is_best_model = False
            if avg_val_loss < best_val_loss - min_delta:
                best_val_loss = avg_val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
                best_epoch = epoch + 1
                is_best_model = True
                print(f"\n*** å‘ç°æ›´å¥½çš„æ¨¡å‹! éªŒè¯æŸå¤±: {avg_val_loss:.6f} ***")
            else:
                patience_counter += 1
                print(f"\næ—©åœè®¡æ•°å™¨: {patience_counter}/{patience}")
            
            # Epochæ€»ç»“
            print(f'\n{"="*60}')
            print(f'Epoch {epoch+1}/{epochs} å®Œæˆ!')
            print(f'è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}')
            print(f'éªŒè¯æŸå¤±: {avg_val_loss:.6f}')
            print(f'éªŒè¯å‡†ç¡®ç‡: {val_acc:.6f} ({val_acc*100:.2f}%)')
            print(f'æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})')
            print(f'{"="*60}')
            
            # ä¿å­˜æ£€æŸ¥ç‚¹å’Œå›¾è¡¨ï¼ˆæ¯10ä¸ªepochæˆ–æœ€ä½³æ¨¡å‹ï¼‰
            if (epoch + 1) % 10 == 0 or is_best_model:
                save_checkpoint_and_plot(epoch + 1, model, optimizer, avg_train_loss, avg_val_loss, val_acc, is_best_model)
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"\næ—©åœè§¦å‘! éªŒè¯æŸå¤±è¿ç»­ {patience} ä¸ªepochæ²¡æœ‰æ”¹å–„")
                print(f"æœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_epoch} epoch, éªŒè¯æŸå¤±: {best_val_loss:.6f}")
                break
    
    except KeyboardInterrupt:
        print("\n\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­!")
        save_interrupted = input("æ˜¯å¦ä¿å­˜å½“å‰æ¨¡å‹? (y/n): ")
        if save_interrupted.lower() == 'y':
            interrupted_path = os.path.join(save_dir, 'interrupted_model.pth')
            torch.save(model.state_dict(), interrupted_path)
            print(f"æ¨¡å‹å·²ä¿å­˜ä¸º: {interrupted_path}")
    
    finally:
        # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œå›¾è¡¨
        # å¦‚æœæœ‰æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
            print(f"\nä½¿ç”¨æœ€ä½³æ¨¡å‹çŠ¶æ€ (Epoch {best_epoch})")
        
        final_model_path = os.path.join(save_dir, 'transformer_sleep_model_final.pth')
        torch.save(model.state_dict(), final_model_path)
        print(f"\næœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ä¸º: {final_model_path}")
        
        # ä¿å­˜æœ€ç»ˆçš„è®­ç»ƒæ›²çº¿å›¾
        final_plot_path = os.path.join(save_dir, 'final_training_curves.png')
        plt.savefig(final_plot_path, dpi=300, bbox_inches='tight')
        print(f"æœ€ç»ˆè®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜ä¸º: {final_plot_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²æ•°æ®
        training_history = {
            'train_losses': train_losses_history,
            'val_losses': val_losses_history,
            'val_accuracies': val_accs_history,
            'epochs': epochs_completed,
            'best_epoch': best_epoch,
            'best_val_loss': best_val_loss,
            'early_stopped': patience_counter >= patience
        }
        
        history_path = os.path.join(save_dir, 'training_history.pkl')
        with open(history_path, 'wb') as f:
            pickle.dump(training_history, f)
        print(f"è®­ç»ƒå†å²å·²ä¿å­˜ä¸º: {history_path}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if train_losses_history:
            print(f"\n{'='*60}")
            print("è®­ç»ƒå®Œæˆç»Ÿè®¡:")
            print(f"æ€»è®­ç»ƒè½®æ•°: {len(train_losses_history)}")
            print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f} (Epoch {best_epoch})")
            print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_accs_history):.6f} (Epoch {val_accs_history.index(max(val_accs_history))+1})")
            print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses_history[-1]:.6f}")
            print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses_history[-1]:.6f}")
            print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accs_history[-1]:.6f}")
            if patience_counter >= patience:
                print(f"æ—©åœè§¦å‘: æ˜¯")
            else:
                print(f"æ—©åœè§¦å‘: å¦")
            print(f"æ‰€æœ‰æ–‡ä»¶ä¿å­˜ä½ç½®: {save_dir}")
            print(f"{'='*60}")
        
        plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼
        plt.show()  # ä¿æŒå›¾è¡¨æ˜¾ç¤º
    
    return model


def predict_realtime(model_path, csv_file, max_predict_samples=None, print_interval=1):
    """å®æ—¶é¢„æµ‹å‡½æ•° - 3ä¸ªç¡çœ é˜¶æ®µ
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        csv_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        max_predict_samples: æœ€å¤§é¢„æµ‹æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºé¢„æµ‹æ‰€æœ‰
        print_interval: æ‰“å°é—´éš”ï¼Œ1è¡¨ç¤ºæ¯ç§’æ‰“å°ï¼Œ10è¡¨ç¤ºæ¯10ç§’æ‰“å°
    """
    
    # åŠ è½½æ¨¡å‹
    model = LSTMTransformerSleepNet(input_size=5, num_classes=3)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    
    # ä½ çš„3ä¸ªæ ‡ç­¾å¯¹åº”å…³ç³»
    stage_names = ['æ¸…é†’', 'æµ…ç¡çœ ', 'æ·±ç¡çœ ']
    
    # å®æ—¶é¢„æµ‹ç¤ºä¾‹ - ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„scaler
    dataset = SimpleSleepDataset(csv_file, window_size=60, step_size=1, max_samples=1000, scaler_path='sleep_scaler.pkl')
    
    predictions = []
    confidences = []
    
    # ç¡®å®šé¢„æµ‹æ•°é‡
    if max_predict_samples is None:
        predict_count = len(dataset)
    else:
        predict_count = min(max_predict_samples, len(dataset))
    
    print(f"å¼€å§‹é¢„æµ‹ï¼Œæ€»å…±{predict_count}ä¸ªæ ·æœ¬ï¼Œæ¯{print_interval}ç§’æ‰“å°ä¸€æ¬¡ç»“æœ")
    print("-" * 60)
    
    with torch.no_grad():
        for i in range(predict_count):
            sample, _ = dataset[i]
            sample = sample.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            output = model(sample)
            predicted_class = torch.argmax(output, dim=1).item()
            confidence = torch.softmax(output, dim=1)[0, predicted_class].item()
            
            predictions.append({
                'second': i,
                'stage': stage_names[predicted_class],
                'stage_code': predicted_class,
                'confidence': confidence,
                'timestamp': f"ç¬¬{i}ç§’"
            })
            
            confidences.append(confidence)
            
            # æ ¹æ®è®¾å®šé—´éš”æ‰“å°ç»“æœ
            if i % print_interval == 0:
                print(f"ç¬¬{i}ç§’: {stage_names[predicted_class]} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                progress = (i + 1) / predict_count * 100
                print(f"è¿›åº¦: {i+1}/{predict_count} ({progress:.1f}%)")
    
    # ç»Ÿè®¡é¢„æµ‹ç»“æœ
    stage_counts = {}
    for pred in predictions:
        stage = pred['stage']
        stage_counts[stage] = stage_counts.get(stage, 0) + 1
    
    print(f"\né¢„æµ‹æ€»ç»“:")
    print(f"é¢„æµ‹æ ·æœ¬æ•°: {len(predictions)}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.3f}")
    print(f"ç¡çœ é˜¶æ®µåˆ†å¸ƒ:")
    for stage, count in stage_counts.items():
        percentage = count/len(predictions)*100
        print(f"  {stage}: {count}æ¬¡ ({percentage:.1f}%)")
    
    return predictions


def inference_single_sample_(sample_data, model, scaler, use_raw_features):
    """
    é¢„æµ‹å•ä¸ªæ ·æœ¬çš„ç¡çœ é˜¶æ®µ - æ”¯æŒåŸå§‹ç‰¹å¾è¾“å…¥
    
    Args:
        sample_data: numpy array
                    - å¦‚æœ use_raw_features=True: [30, 3] (breath_line, heart_line, signal_intensity)
                    - å¦‚æœ use_raw_features=False: [30, 5] (å·²å¤„ç†çš„5ä¸ªç‰¹å¾)
        model: å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ (SimpleSleepNet)
        scaler: å·²åŠ è½½çš„æ ‡å‡†åŒ–å™¨å®ä¾‹ (StandardScaler)
        use_raw_features: æ˜¯å¦ä½¿ç”¨åŸå§‹3ä¸ªç‰¹å¾è¾“å…¥
    
    Returns:
        dict: é¢„æµ‹ç»“æœ
    """
    
    try:
        # 1. è¾“å…¥éªŒè¯å’Œç‰¹å¾å·¥ç¨‹
        if not isinstance(sample_data, np.ndarray):
            raise TypeError(f"è¾“å…¥æ•°æ®å¿…é¡»æ˜¯numpyæ•°ç»„ï¼Œå®é™…ç±»å‹: {type(sample_data)}")
        
        if use_raw_features:
            # åŸå§‹3ä¸ªç‰¹å¾è¾“å…¥ [30, 3]
            if sample_data.shape != (30, 3):
                raise ValueError(f"åŸå§‹ç‰¹å¾è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸º (30, 3)ï¼Œå®é™…ä¸º {sample_data.shape}")
            
            # è¿›è¡Œç‰¹å¾å·¥ç¨‹
            processed_data = _perform_feature_engineering(sample_data)
        else:
            # å·²å¤„ç†çš„5ä¸ªç‰¹å¾è¾“å…¥ [30, 5]
            if sample_data.shape != (30, 5):
                raise ValueError(f"å¤„ç†åç‰¹å¾è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸º (30, 5)ï¼Œå®é™…ä¸º {sample_data.shape}")
            processed_data = sample_data
        
        # 2. æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ï¼‰
        normalized_data = scaler.transform(processed_data)
        tensor_data = torch.FloatTensor(normalized_data).unsqueeze(0).to("cuda:0")  # [1, 30, 5]
        
        # 3. æ¨¡å‹é¢„æµ‹
        model.eval()
        with torch.no_grad():
            output = model(tensor_data)  # [1, 3]
            probabilities = torch.softmax(output, dim=1)  # [1, 3]
            predicted_class = torch.argmax(output, dim=1).item()  # 0, 1, or 2
            confidence = probabilities[0, predicted_class].item()  # ç½®ä¿¡åº¦
        
        
        return predicted_class, confidence
        
    except Exception as e:
        raise ValueError(f"fail to exec deep learning model! {str(e)}") from e


def load_sleep_scaler(scaler_path='sleep_scaler.pkl'):
    """
    åŠ è½½æ ‡å‡†åŒ–å™¨
    
    Args:
        scaler_path: æ ‡å‡†åŒ–å™¨æ–‡ä»¶è·¯å¾„
    
    Returns:
        StandardScaler: å·²åŠ è½½çš„æ ‡å‡†åŒ–å™¨å®ä¾‹
    """
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶: {scaler_path}")
    
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä» {scaler_path} åŠ è½½")
    return scaler


def _perform_feature_engineering(raw_data):
    """
    å¯¹åŸå§‹3ä¸ªç‰¹å¾è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œç”Ÿæˆ5ä¸ªç‰¹å¾
    
    Args:
        raw_data: numpy array [30, 3] - (breath_line, heart_line, signal_intensity)
    
    Returns:
        numpy array [30, 5] - ç‰¹å¾å·¥ç¨‹åçš„5ä¸ªç‰¹å¾
    """
    import pandas as pd
    
    # è½¬æ¢ä¸ºDataFrameæ–¹ä¾¿å¤„ç†
    df = pd.DataFrame(raw_data, columns=['breath_line', 'heart_line', 'signal_intensity'])
    
    # ç‰¹å¾å·¥ç¨‹ (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
    df['heart_rate'] = df['heart_line'].rolling(10, min_periods=1).std() * 100 + 70
    df['resp_rate'] = df['breath_line'].rolling(10, min_periods=1).std() * 50 + 15
    df['signal_quality'] = df['signal_intensity'] / 50.0
    
    # 5ä¸ªç‰¹å¾çš„é¡ºåºï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    features = ['resp_rate', 'heart_rate', 'breath_line', 'heart_line', 'signal_quality']
    
    # å¤„ç†ç¼ºå¤±å€¼
    df[features] = df[features].ffill().fillna(0)
    
    return df[features].values


def set_seed(seed=42):
    """å›ºå®šæ‰€æœ‰å¯èƒ½çš„éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°"""
    # Pythonå†…ç½®éšæœºåº“
    random.seed(seed)
    # NumPy
    np.random.seed(seed)
    # PyTorch CPU
    torch.manual_seed(seed)
    # PyTorch GPUï¼ˆå•å¡ï¼‰
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # å¤šå¡ç¯å¢ƒ
    # CuDNNï¼ˆç¡®ä¿å·ç§¯æ“ä½œç¡®å®šæ€§ï¼‰
    torch.backends.cudnn.deterministic = True  # å›ºå®šå·ç§¯ç®—æ³•
    torch.backends.cudnn.benchmark = False     # ç¦ç”¨è‡ªåŠ¨é€‰æ‹©æœ€å¿«ç®—æ³•ï¼ˆå¯èƒ½å¼•å…¥éšæœºæ€§ï¼‰


def process_data(label_studio_json_file: str):
    def read_json_file(file_path):
        """è¯»å–JSONæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    
    json_data = read_json_file(label_studio_json_file)
    return json_data


def parse_datetime(date_str):
    """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
    try:
        return pd.to_datetime(date_str, format='%Y/%m/%d %H:%M:%S')
    except:
        try:
            return pd.to_datetime(date_str)
        except:
            return None


def process_csv_with_time_labels(config_data, base_path="/work/ai/WHOAMI/train_data/vital_sleep_classifier", output_path="merged_time_labeled_data.csv"):
    """
    ä¿®å¤ç‰ˆæœ¬ï¼šæ ¹æ®æ—¶é—´åŒºé—´ç»™CSVæ•°æ®åˆ†é…æ ‡ç­¾
    """
    
    def parse_datetime(date_str):
        """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
        try:
            return pd.to_datetime(date_str, format='%Y/%m/%d %H:%M:%S')
        except:
            try:
                return pd.to_datetime(date_str)
            except:
                return None
    
    all_processed_data = []
    
    # ç›´æ¥å¤„ç†æ¯ä¸ªé…ç½®é¡¹
    for item in config_data:
        csv_path = item['csv']
        csv_filename = csv_path.split('-')[-1]  # æå–æ–‡ä»¶å
        full_path = os.path.join(base_path, csv_filename)
        
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {csv_filename}")
        
        if not os.path.exists(full_path):
            print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            continue
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(full_path)
        print(f"  ğŸ“Š æ–‡ä»¶è¡Œæ•°: {len(df)}")

        df = df.dropna()  # åˆ é™¤NaNè¡Œ
        df = df[~((df['breath_line'] == 0) & (df['heart_line'] == 0))]  # åˆ é™¤åŒæ—¶ä¸º0çš„è¡Œ
        
        # è·å–æ—¶é—´åˆ—ï¼ˆå‡è®¾æ˜¯ç¬¬ä¸€åˆ—ï¼‰
        time_column = df.columns[0]
        print(f"  ğŸ“… æ—¶é—´åˆ—: {time_column}")
        
        # è½¬æ¢æ—¶é—´åˆ—
        df[time_column] = pd.to_datetime(df[time_column], errors='coerce')
        
        # æ·»åŠ æ–°åˆ— - åˆ†åˆ«å¤„ç†ï¼Œé¿å…æ··æ·†
        df_copy = df.copy()
        df_copy['label'] = 'unlabeled'  # é»˜è®¤æ ‡ç­¾
        df_copy['source_file'] = csv_filename  # æ–‡ä»¶æ¥æº
        
        print(f"  ğŸ”§ åˆå§‹åŒ–åçš„åˆ—æ•°: {len(df_copy.columns)}")
        
        # å¤„ç†æ ‡ç­¾åŒºé—´
        label_data = item.get('label', [])
        if not isinstance(label_data, list):
            label_data = [label_data]
        
        for idx, label_info in enumerate(label_data):
            start_time = parse_datetime(label_info['start'])
            end_time = parse_datetime(label_info['end'])
            labels = label_info.get('timeserieslabels', [])
            
            if not labels:
                continue
                
            label_text = str(labels[0]).strip()  # ç¡®ä¿æ˜¯å¹²å‡€çš„å­—ç¬¦ä¸²
            
            print(f"  ğŸ·ï¸  åŒºé—´ {idx+1}: {label_info['start']} - {label_info['end']}")
            print(f"      æ ‡ç­¾: '{label_text}'")
            
            if start_time is None or end_time is None:
                print(f"      âš ï¸ æ—¶é—´è§£æå¤±è´¥")
                continue
            
            # æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            time_mask = (df_copy[time_column] >= start_time) & (df_copy[time_column] <= end_time)
            matched_count = time_mask.sum()
            
            if matched_count > 0:
                # é‡è¦ï¼šç¡®ä¿åªè®¾ç½®labelåˆ—ï¼Œä¸å½±å“å…¶ä»–åˆ—
                df_copy.loc[time_mask, 'label'] = label_text
                print(f"      âœ… åŒ¹é… {matched_count} è¡Œæ•°æ®")
            else:
                print(f"      âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ—¶é—´æ•°æ®")
        
        # éªŒè¯ç»“æœ
        print(f"  ğŸ” æ ‡ç­¾ç»Ÿè®¡: {df_copy['label'].value_counts().to_dict()}")
        print(f"  ğŸ“‚ æ–‡ä»¶æ¥æº: {df_copy['source_file'].iloc[0] if len(df_copy) > 0 else 'None'}")
        df_copy = df_copy[df_copy['label'] != "å¼‚å¸¸"]
        all_processed_data.append(df_copy)
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_processed_data:
        print(f"\nğŸ”„ åˆå¹¶ {len(all_processed_data)} ä¸ªæ–‡ä»¶...")
        final_df = pd.concat(all_processed_data, ignore_index=True)
        
        # æœ€ç»ˆéªŒè¯
        print(f"ğŸ“Š åˆå¹¶åæ€»è¡Œæ•°: {len(final_df)}")
        print(f"ğŸ“‹ æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in final_df['label'].value_counts().items():
            print(f"  '{label}': {count} è¡Œ")
        
        print(f"ğŸ“‚ æ–‡ä»¶æ¥æºåˆ†å¸ƒ:")
        for source, count in final_df['source_file'].value_counts().items():
            print(f"  '{source}': {count} è¡Œ")
        
        # ä¿å­˜æ–‡ä»¶
        final_df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"\nâœ… ä¿å­˜å®Œæˆ: {output_path}")
        
        return final_df
    else:
        print("âŒ æ²¡æœ‰å¤„ç†ä»»ä½•æ•°æ®")
        return None



def verify_label(csv_file, config_data, sample_count):
    from datetime import datetime
    def parse_flexible_time(time_str):
        """çµæ´»è§£æä¸åŒæ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²"""
        formats = [
            "%Y-%m-%d %H:%M:%S",  # 2025-06-23 06:51:23
            "%Y/%m/%d %H:%M:%S",  # 2025/06/23 06:51:23
            "%Y-%m-%d %H:%M",     # 2025-06-23 06:51
            "%Y/%m/%d %H:%M"      # 2025/06/23 06:51
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(time_str, fmt)
            except ValueError:
                continue
        
        raise ValueError(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}")
    result = []
    array_data = np.array(pd.read_csv(csv_file, encoding='utf-8'))
    sample_data = array_data[np.random.choice(a=len(array_data), size=sample_count, replace=False)]
    verify_data = sample_data[:, [0, -2, -1]]
    for item in verify_data:
        for item_config in config_data:
            if item_config["csv"].split("-")[-1] == item[-1]:
                for label_item in item_config["label"]:
                    target_time = parse_flexible_time(item[0])
                    start_time = parse_datetime(label_item["start"])
                    end_time = parse_datetime(label_item["end"])
                    if start_time <= target_time <= end_time:
                        if item[1] == label_item["timeserieslabels"][0]:
                            result.append(1)
                        else:
                            result.append(0)
                break
    print(sum(result))
    return sum(result) / len(result) if len(result) > 0 else 0, array_data


# ä½¿ç”¨ç¤ºä¾‹ - é€‚é…ä½ çš„æ•°æ®
if __name__ == "__main__":

    # è¯»å–æ•°æ®
    json_data = process_data("/work/ai/WHOAMI/train_data/eee/project-4-at-2025-07-28-07-16-c32546d7.json")
    print(json_data)
    output_path="/work/ai/WHOAMI/train_data/vital_sleep_classifier/out.csv"
    process_csv_with_time_labels(config_data=json_data, output_path=output_path)

    # éªŒè¯æ•°æ®æ ‡æ³¨å‡†ç¡®ç‡
    precision, array_data = verify_label(csv_file=output_path, config_data=json_data, sample_count=1000)
    print(precision)
    
    print(len(array_data))
    
    set_seed(42)
    # ä½ çš„CSVæ–‡ä»¶è·¯å¾„
    csv_file = output_path
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ3ç±»ç¡çœ åˆ†æœŸæ¨¡å‹...")
    print("="*50)
    
    # # å¿«é€Ÿæµ‹è¯•é€‰é¡¹ - å¦‚æœæ•°æ®å¤ªå¤§ï¼Œå¯ä»¥å…ˆç”¨å°æ ·æœ¬æµ‹è¯•
    USE_QUICK_TEST = False  # æ”¹ä¸ºFalseä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ŒTrueä½¿ç”¨éƒ¨åˆ†æ•°æ®æµ‹è¯•
    max_samples = 50000 if USE_QUICK_TEST else None  # å¿«é€Ÿæµ‹è¯•ä½¿ç”¨5ä¸‡è¡Œæ•°æ®
    
    if USE_QUICK_TEST:
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ - ä½¿ç”¨éƒ¨åˆ†æ•°æ®")
    else:
        print("ğŸŒ å®Œæ•´è®­ç»ƒæ¨¡å¼ - ä½¿ç”¨å…¨éƒ¨æ•°æ®")
    
    # 1. åŠ è½½æ•°æ®
    dataset = SimpleSleepDataset(csv_file, window_size=60, step_size=30, max_samples=max_samples, scaler_path='sleep_scaler.pkl')
    
    train_param_json = [
        {"name": "7_bi_transformer", "input_size": 5, "bidirectional_flag": True, "transformer_flag": True},
        {"name": "7_bi", "input_size": 5, "bidirectional_flag": True, "transformer_flag": False},
        {"name": "7_transformer", "input_size": 5, "bidirectional_flag": False, "transformer_flag": True},
        {"name": "7", "input_size": 5, "bidirectional_flag": False, "transformer_flag": False},
    ]
    
    for item in train_param_json:
        try:
            # è®­ç»ƒæ¨¡å‹
            model = train_simple_model(
                dataset=dataset, 
                epochs=150, 
                max_samples=max_samples, 
                param_name=item["name"],
                input_size=item["input_size"],
                bidirectional_flag=item["bidirectional_flag"],
                transformer_flag=item["transformer_flag"],
            )
            print("âœ… è®­ç»ƒå®Œæˆï¼")
            
            print("\nğŸ”® å¼€å§‹å®æ—¶é¢„æµ‹...")
            print("="*50)
            
            # é¢„æµ‹é€‰é¡¹
            print("é€‰æ‹©é¢„æµ‹æ¨¡å¼:")
            print("1. æ¯ç§’é¢„æµ‹ - é¢„æµ‹100ç§’")
            print("2. æ¯10ç§’é¢„æµ‹ - é¢„æµ‹1000ç§’") 
            print("3. å¿«é€Ÿé¢„æµ‹ - é¢„æµ‹æ‰€æœ‰å¯èƒ½çš„æ ·æœ¬")
            
            # ä¸åŒé¢„æµ‹æ¨¡å¼
            # æ¨¡å¼1: æ¯ç§’æ˜¾ç¤ºï¼Œé¢„æµ‹100ç§’
            # print("\nğŸ“Š æ¨¡å¼1: æ¯ç§’é¢„æµ‹ç»“æœ")
            # predictions_1s = predict_realtime('transformer_sleep_model.pth', csv_file, 
            #                                 max_predict_samples=100, print_interval=1)
            
            # print("\nğŸ“Š æ¨¡å¼2: æ¯10ç§’é¢„æµ‹ç»“æœ")  
            # predictions_10s = predict_realtime('transformer_sleep_model.pth', csv_file,
            #                                 max_predict_samples=1000, print_interval=60)
            
            # print(f"âœ… é¢„æµ‹å®Œæˆ")
            
            # # ä¿å­˜é¢„æµ‹ç»“æœ
            # with open('transformer_sleep_predictions_1s.json', 'w', encoding='utf-8') as f:
            #     json.dump(predictions_1s, f, ensure_ascii=False, indent=2)
            
            # with open('transformer_sleep_predictions_10s.json', 'w', encoding='utf-8') as f:
            #     json.dump(predictions_10s, f, ensure_ascii=False, indent=2)
                
            # print("ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: sleep_predictions_1s.json å’Œ sleep_predictions_10s.json")
            
        except FileNotFoundError:
            print("âŒ æ‰¾ä¸åˆ°CSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ï¼")
            print("å½“å‰æŸ¥æ‰¾æ–‡ä»¶:", csv_file)
            print("è¯·å°†ä½ çš„CSVæ–‡ä»¶æ”¾åœ¨è„šæœ¬åŒç›®å½•ä¸‹ï¼Œæˆ–ä¿®æ”¹csv_fileå˜é‡")

        except Exception as e:
            print(f"âŒ å‡ºé”™äº†: {e}")
            import traceback
            traceback.print_exc()
            print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®")
            print("ç¡®ä¿CSVåŒ…å«åˆ—: create_time, breath_line, heart_line, distance, signal_intensity, label")


    """
    # predict
    import numpy as np

    # ç”Ÿæˆ30ç§’ Ã— 3ä¸ªç‰¹å¾çš„æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
    sample_data = np.random.randn(30, 3)

    # æˆ–è€…ç”Ÿæˆæ›´çœŸå®çš„ä¼ æ„Ÿå™¨æ•°æ®èŒƒå›´
    sample_data = np.array([
        np.random.normal(0, 1, 30),      # breath_line: å‘¼å¸ä¿¡å· (å‡å€¼0, æ ‡å‡†å·®1)
        np.random.normal(0, 1, 30),      # heart_line: å¿ƒç”µä¿¡å· (å‡å€¼0, æ ‡å‡†å·®1)  
        np.random.uniform(20, 80, 30)    # signal_intensity: ä¿¡å·å¼ºåº¦ (20-80èŒƒå›´)
    ]).T  # è½¬ç½®ä¸º (30, 3)
    scaler = load_sleep_scaler(scaler_path="/work/ai/WHOAMI/sleep_scaler.pkl")
    model = SimpleSleepNet(
        input_size=5,
        seq_length=30, 
        num_classes=3,
    )
    
    # checkpoint = torch.load(self.model_path, map_location=self.device)
    model.load_state_dict(torch.load("/work/ai/WHOAMI/simple_sleep_model.pth"))
    print(sample_data)
    result, confidence = inference_single_sample_(sample_data=sample_data, model=model, scaler=scaler, use_raw_features=True)
    print(result, confidence)
    """