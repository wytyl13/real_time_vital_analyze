import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import json
import pickle  # ç”¨äºä¿å­˜scaler
import os  # ç”¨äºæ–‡ä»¶è·¯å¾„æ£€æŸ¥

class SimpleSleepDataset(Dataset):
    """ç®€å•çš„ç¡çœ æ•°æ®é›† - 3ä¸ªç¡çœ é˜¶æ®µ"""
    
    def __init__(self, csv_file, window_size=30, step_size=1, max_samples=None, scaler_path=None):
        # è¯»å–æ•°æ®
        print("æ­£åœ¨è¯»å–CSVæ–‡ä»¶...")
        self.df = pd.read_csv(csv_file, encoding='gbk')
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œåˆ™åªä½¿ç”¨éƒ¨åˆ†æ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        if max_samples and len(self.df) > max_samples:
            print(f"ä½¿ç”¨å‰ {max_samples} è¡Œæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•...")
            self.df = self.df.head(max_samples)
        
        self.window_size = window_size
        
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")
        print(f"åˆ—å: {self.df.columns.tolist()}")
        
        # å¤„ç†ä¸­æ–‡æ ‡ç­¾ - ä½ çš„3ä¸ªæ ‡ç­¾
        label_mapping = {
            'æ¸…é†’': 0,    # Wake
            'æµ…ç¡çœ ': 1,  # Light Sleep 
            'æ·±ç¡çœ ': 2,  # Deep Sleep
            # å¦‚æœæœ‰å…¶ä»–å˜ä½“å†™æ³•ï¼Œä¹Ÿå¯ä»¥æ˜ å°„
            'æ¸…é†’çŠ¶æ€': 0,
            'æµ…åº¦ç¡çœ ': 1,
            'æ·±åº¦ç¡çœ ': 2,
        }
        
        # æ ¹æ®ä½ çš„æ•°æ®ï¼Œlabelåˆ—åŒ…å«ä¸­æ–‡
        if 'label' in self.df.columns:
            # æ˜ å°„ä¸­æ–‡æ ‡ç­¾ä¸ºæ•°å­—
            self.df['label_num'] = self.df['label'].map(label_mapping)
            # å¦‚æœæœ‰æœªæ˜ å°„çš„æ ‡ç­¾ï¼Œè®¾ä¸º0ï¼ˆæ¸…é†’ï¼‰
            self.df['label_num'] = self.df['label_num'].fillna(0).astype(int)
            print(f"æ ‡ç­¾æ˜ å°„: {self.df['label'].value_counts()}")
        else:
            self.df['label_num'] = 0
        
        # ä½¿ç”¨ä½ çš„å®é™…åˆ—åï¼šbreath_line, heart_line
        # æ·»åŠ é¢å¤–ç‰¹å¾æå‡æ•ˆæœ
        print("æ­£åœ¨è®¡ç®—ç‰¹å¾å·¥ç¨‹...")
        self.df['heart_rate'] = self.df['heart_line'].rolling(10, min_periods=1).std() * 100 + 70
        self.df['resp_rate'] = self.df['breath_line'].rolling(10, min_periods=1).std() * 50 + 15
        self.df['signal_quality'] = self.df['signal_intensity'] / 50.0  # å½’ä¸€åŒ–ä¿¡å·å¼ºåº¦
        print("ç‰¹å¾å·¥ç¨‹å®Œæˆ!")
        
        # 5ä¸ªç‰¹å¾ï¼šå‘¼å¸ç‡ã€å¿ƒç‡ã€å‘¼å¸çº¿ã€å¿ƒçº¿ã€ä¿¡å·è´¨é‡
        features = ['resp_rate', 'heart_rate', 'breath_line', 'heart_line', 'signal_quality']
        
        # ä¿®å¤ï¼šå¤„ç†ç¼ºå¤±å€¼ - ä½¿ç”¨æ–°çš„pandasè¯­æ³•
        print("å¤„ç†ç¼ºå¤±å€¼...")
        self.df[features] = self.df[features].ffill().fillna(0)
        
        # å…³é”®ä¿®å¤ï¼šæ ‡å‡†åŒ–å¤„ç†
        if scaler_path and os.path.exists(scaler_path):
            # é¢„æµ‹æ—¶ï¼šåŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„scaler
            print(f"åŠ è½½é¢„è®­ç»ƒçš„scaler: {scaler_path}")
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            self.df[features] = scaler.transform(self.df[features])
            print("ä½¿ç”¨é¢„è®­ç»ƒscalerå®Œæˆæ•°æ®æ ‡å‡†åŒ–!")
        else:
            # è®­ç»ƒæ—¶ï¼šåˆ›å»ºæ–°çš„scalerå¹¶ä¿å­˜
            print("åˆ›å»ºæ–°çš„scalerè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
            scaler = StandardScaler()
            self.df[features] = scaler.fit_transform(self.df[features])
            # ä¿å­˜scalerä¾›é¢„æµ‹æ—¶ä½¿ç”¨
            scaler_save_path = scaler_path if scaler_path else 'sleep_scaler.pkl'
            with open(scaler_save_path, 'wb') as f:
                pickle.dump(scaler, f)
            print(f"æ•°æ®æ ‡å‡†åŒ–å®Œæˆ! Scalerå·²ä¿å­˜åˆ°: {scaler_save_path}")
        
        # æ‰“å°æ ‡å‡†åŒ–åçš„æ•°æ®ç»Ÿè®¡
        print("æ ‡å‡†åŒ–åçš„ç‰¹å¾ç»Ÿè®¡:")
        for feature in features:
            mean_val = self.df[feature].mean()
            std_val = self.df[feature].std()
            print(f"  {feature}: å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}")
        
        # åˆ›å»ºçª—å£ - æ·»åŠ è¿›åº¦æ˜¾ç¤º
        print("å¼€å§‹åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬...")
        self.windows = []
        self.labels = []
        
        total_windows = (len(self.df) - window_size) // step_size + 1
        print(f"é¢„è®¡åˆ›å»º {total_windows} ä¸ªçª—å£æ ·æœ¬...")
        
        for i in range(0, len(self.df) - window_size + 1, step_size):
            window = self.df[features].iloc[i:i+window_size].values
            label = self.df['label_num'].iloc[i+window_size-1]
            
            self.windows.append(window)
            self.labels.append(label)
            
            # æ¯1000ä¸ªçª—å£æ‰“å°ä¸€æ¬¡è¿›åº¦
            if len(self.windows) % 1000 == 0:
                progress = len(self.windows) / total_windows * 100
                print(f"è¿›åº¦: {len(self.windows)}/{total_windows} ({progress:.1f}%)")
        
        self.windows = np.array(self.windows)
        self.labels = np.array(self.labels)
        
        print(f"åˆ›å»ºäº† {len(self.windows)} ä¸ªæ ·æœ¬")
        print(f"æ•°æ®å½¢çŠ¶: {self.windows.shape}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.labels)}")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒè¯¦æƒ…
        unique_labels, counts = np.unique(self.labels, return_counts=True)
        label_names = ['æ¸…é†’', 'æµ…ç¡çœ ', 'æ·±ç¡çœ ']  # 3ä¸ªæ ‡ç­¾
        for label, count in zip(unique_labels, counts):
            label_name = label_names[label] if label < len(label_names) else f'æ ‡ç­¾{label}'
            print(f"  {label_name}: {count}ä¸ªæ ·æœ¬ ({count/len(self.labels)*100:.1f}%)")
    
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, idx):
        # ä¿®å¤ï¼šç›´æ¥è¿”å›æ ‡é‡æ ‡ç­¾ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.labels[idx]]).squeeze()

class SimpleSleepNet(nn.Module):
    """è¶…ç®€å•çš„ç¡çœ åˆ†æœŸç½‘ç»œ - 3ä¸ªç¡çœ é˜¶æ®µ"""
    
    def __init__(self, input_size=5, seq_length=30, num_classes=3):  # 3ä¸ªç±»åˆ«ï¼šæ¸…é†’ã€æµ…ç¡çœ ã€æ·±ç¡çœ 
        super().__init__()
        
        # ç®€å•çš„CNN+LSTM
        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool1d(1)
        
        # åŒå‘LSTMæå‡æ•ˆæœ - ä¿®å¤dropoutè­¦å‘Š
        self.lstm = nn.LSTM(128, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
        
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),  # åŒå‘LSTMè¾“å‡º128ç»´
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(), 
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)  # 3ä¸ªè¾“å‡º
        )
    
    def forward(self, x):
        # x: [batch, seq_len, features] -> [batch, features, seq_len]
        x = x.transpose(1, 2)
        
        # æ›´æ·±çš„CNNæå–ç‰¹å¾
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = self.pool(x).squeeze(-1)  # [batch, 128]
        
        # ä¸ºLSTMæ·»åŠ åºåˆ—ç»´åº¦
        x = x.unsqueeze(1)  # [batch, 1, 128]
        
        # åŒå‘LSTM
        lstm_out, _ = self.lstm(x)
        
        # åˆ†ç±»
        output = self.classifier(lstm_out[:, -1, :])
        return output


def train_simple_model(csv_file, epochs=50, max_samples=None):
    """è®­ç»ƒå‡½æ•°"""
    
    print("å¼€å§‹è®­ç»ƒç®€å•ç¡çœ åˆ†æœŸæ¨¡å‹...")
    
    # 1. åŠ è½½æ•°æ®
    dataset = SimpleSleepDataset(csv_file, window_size=30, step_size=5, max_samples=max_samples, scaler_path='sleep_scaler.pkl')
    
    # 2. åˆ†å‰²æ•°æ®é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # ä¿®å¤ï¼šæ·»åŠ drop_last=Trueé¿å…ä¸å®Œæ•´çš„batchå¯¼è‡´é”™è¯¯
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}, éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # 3. åˆ›å»ºæ¨¡å‹ - 3ä¸ªç¡çœ é˜¶æ®µ
    model = SimpleSleepNet(input_size=5, num_classes=3)  # 5ä¸ªç‰¹å¾ï¼Œ3ä¸ªç¡çœ ç±»åˆ«
    
    # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - ä¸ºå°‘æ•°ç±»åˆ«åŠ æƒ
    class_counts = np.bincount(dataset.labels)
    class_weights = 1.0 / (class_counts + 1e-6)  # é¿å…é™¤é›¶
    class_weights = torch.FloatTensor(class_weights / class_weights.sum() * len(class_weights))
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # 4. è®­ç»ƒå¾ªç¯
    train_losses = []
    val_accs = []
    val_losses = []
    
    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):
            # ä¿®å¤ï¼šç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
            if batch_y.dim() > 1:
                batch_y = batch_y.squeeze()
            
            # æ£€æŸ¥batchæ˜¯å¦ä¸ºç©º
            if batch_x.size(0) == 0 or batch_y.size(0) == 0:
                continue
                
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # å®æ—¶æ‰“å°è®­ç»ƒæŸå¤±
            print(f"\rEpoch {epoch+1} [{batch_idx+1}/{len(train_loader)}] Train Loss: {loss.item():.4f}", end='')
        
        # éªŒè¯
        model.eval()
        val_correct = 0
        val_total = 0
        val_loss = 0
        
        with torch.no_grad():
            for batch_idx, (batch_x, batch_y) in enumerate(val_loader):
                # ä¿®å¤ï¼šç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
                if batch_y.dim() > 1:
                    batch_y = batch_y.squeeze()
                
                # æ£€æŸ¥batchæ˜¯å¦ä¸ºç©º
                if batch_x.size(0) == 0 or batch_y.size(0) == 0:
                    continue
                    
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()
                
                # å®æ—¶æ‰“å°éªŒè¯æŸå¤±
                print(f"\rEpoch {epoch+1} - Validation [{batch_idx+1}/{len(val_loader)}] Val Loss: {loss.item():.4f}", end='', flush=True)
        
        val_acc = val_correct / val_total
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        train_losses.append(avg_train_loss)
        val_accs.append(val_acc)
        val_losses.append(avg_val_loss)
        
        # æ¯ä¸ªepochç»“æŸåæ‰“å°æ€»ç»“
        print(f'\nEpoch {epoch+1}/{epochs}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}')
    
    # 5. ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'simple_sleep_model.pth')
    print("æ¨¡å‹å·²ä¿å­˜ä¸º: simple_sleep_model.pth")
    
    # 6. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 3, 2)
    plt.plot(val_accs)
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    
    plt.subplot(1, 3, 3)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.plot([x*max(train_losses+val_losses) for x in val_accs], label='Val Acc (scaled)')
    plt.title('All Metrics')
    plt.xlabel('Epoch')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.show()
    
    return model


def predict_realtime(model_path, csv_file, max_predict_samples=None, print_interval=1):
    """å®æ—¶é¢„æµ‹å‡½æ•° - 3ä¸ªç¡çœ é˜¶æ®µ
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        csv_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        max_predict_samples: æœ€å¤§é¢„æµ‹æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºé¢„æµ‹æ‰€æœ‰
        print_interval: æ‰“å°é—´éš”ï¼Œ1è¡¨ç¤ºæ¯ç§’æ‰“å°ï¼Œ10è¡¨ç¤ºæ¯10ç§’æ‰“å°
    """
    
    # åŠ è½½æ¨¡å‹
    model = SimpleSleepNet(input_size=5, num_classes=3)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    
    # ä½ çš„3ä¸ªæ ‡ç­¾å¯¹åº”å…³ç³»
    stage_names = ['æ¸…é†’', 'æµ…ç¡çœ ', 'æ·±ç¡çœ ']
    
    # å®æ—¶é¢„æµ‹ç¤ºä¾‹ - ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„scaler
    dataset = SimpleSleepDataset(csv_file, window_size=30, step_size=1, max_samples=1000, scaler_path='sleep_scaler.pkl')
    
    predictions = []
    confidences = []
    
    # ç¡®å®šé¢„æµ‹æ•°é‡
    if max_predict_samples is None:
        predict_count = len(dataset)
    else:
        predict_count = min(max_predict_samples, len(dataset))
    
    print(f"å¼€å§‹é¢„æµ‹ï¼Œæ€»å…±{predict_count}ä¸ªæ ·æœ¬ï¼Œæ¯{print_interval}ç§’æ‰“å°ä¸€æ¬¡ç»“æœ")
    print("-" * 60)
    
    with torch.no_grad():
        for i in range(predict_count):
            sample, _ = dataset[i]
            sample = sample.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            output = model(sample)
            predicted_class = torch.argmax(output, dim=1).item()
            confidence = torch.softmax(output, dim=1)[0, predicted_class].item()
            
            predictions.append({
                'second': i,
                'stage': stage_names[predicted_class],
                'stage_code': predicted_class,
                'confidence': confidence,
                'timestamp': f"ç¬¬{i}ç§’"
            })
            
            confidences.append(confidence)
            
            # æ ¹æ®è®¾å®šé—´éš”æ‰“å°ç»“æœ
            if i % print_interval == 0:
                print(f"ç¬¬{i}ç§’: {stage_names[predicted_class]} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                progress = (i + 1) / predict_count * 100
                print(f"è¿›åº¦: {i+1}/{predict_count} ({progress:.1f}%)")
    
    # ç»Ÿè®¡é¢„æµ‹ç»“æœ
    stage_counts = {}
    for pred in predictions:
        stage = pred['stage']
        stage_counts[stage] = stage_counts.get(stage, 0) + 1
    
    print(f"\né¢„æµ‹æ€»ç»“:")
    print(f"é¢„æµ‹æ ·æœ¬æ•°: {len(predictions)}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.3f}")
    print(f"ç¡çœ é˜¶æ®µåˆ†å¸ƒ:")
    for stage, count in stage_counts.items():
        percentage = count/len(predictions)*100
        print(f"  {stage}: {count}æ¬¡ ({percentage:.1f}%)")
    
    return predictions


def inference_single_sample_(sample_data, model, scaler, use_raw_features):
    """
    é¢„æµ‹å•ä¸ªæ ·æœ¬çš„ç¡çœ é˜¶æ®µ - æ”¯æŒåŸå§‹ç‰¹å¾è¾“å…¥
    
    Args:
        sample_data: numpy array
                    - å¦‚æœ use_raw_features=True: [30, 3] (breath_line, heart_line, signal_intensity)
                    - å¦‚æœ use_raw_features=False: [30, 5] (å·²å¤„ç†çš„5ä¸ªç‰¹å¾)
        model: å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ (SimpleSleepNet)
        scaler: å·²åŠ è½½çš„æ ‡å‡†åŒ–å™¨å®ä¾‹ (StandardScaler)
        use_raw_features: æ˜¯å¦ä½¿ç”¨åŸå§‹3ä¸ªç‰¹å¾è¾“å…¥
    
    Returns:
        dict: é¢„æµ‹ç»“æœ
    """
    
    try:
        # 1. è¾“å…¥éªŒè¯å’Œç‰¹å¾å·¥ç¨‹
        if not isinstance(sample_data, np.ndarray):
            raise TypeError(f"è¾“å…¥æ•°æ®å¿…é¡»æ˜¯numpyæ•°ç»„ï¼Œå®é™…ç±»å‹: {type(sample_data)}")
        
        if use_raw_features:
            # åŸå§‹3ä¸ªç‰¹å¾è¾“å…¥ [30, 3]
            if sample_data.shape != (30, 3):
                raise ValueError(f"åŸå§‹ç‰¹å¾è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸º (30, 3)ï¼Œå®é™…ä¸º {sample_data.shape}")
            
            # è¿›è¡Œç‰¹å¾å·¥ç¨‹
            processed_data = _perform_feature_engineering(sample_data)
        else:
            # å·²å¤„ç†çš„5ä¸ªç‰¹å¾è¾“å…¥ [30, 5]
            if sample_data.shape != (30, 5):
                raise ValueError(f"å¤„ç†åç‰¹å¾è¾“å…¥å½¢çŠ¶å¿…é¡»ä¸º (30, 5)ï¼Œå®é™…ä¸º {sample_data.shape}")
            processed_data = sample_data
        
        # 2. æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ï¼‰
        normalized_data = scaler.transform(processed_data)
        tensor_data = torch.FloatTensor(normalized_data).unsqueeze(0).to("cuda:0")  # [1, 30, 5]
        
        # 3. æ¨¡å‹é¢„æµ‹
        model.eval()
        with torch.no_grad():
            output = model(tensor_data)  # [1, 3]
            probabilities = torch.softmax(output, dim=1)  # [1, 3]
            predicted_class = torch.argmax(output, dim=1).item()  # 0, 1, or 2
            confidence = probabilities[0, predicted_class].item()  # ç½®ä¿¡åº¦
        
        
        return predicted_class, confidence
        
    except Exception as e:
        raise ValueError(f"fail to exec deep learning model! {str(e)}") from e


def load_sleep_scaler(scaler_path='sleep_scaler.pkl'):
    """
    åŠ è½½æ ‡å‡†åŒ–å™¨
    
    Args:
        scaler_path: æ ‡å‡†åŒ–å™¨æ–‡ä»¶è·¯å¾„
    
    Returns:
        StandardScaler: å·²åŠ è½½çš„æ ‡å‡†åŒ–å™¨å®ä¾‹
    """
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶: {scaler_path}")
    
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä» {scaler_path} åŠ è½½")
    return scaler


def _perform_feature_engineering(raw_data):
    """
    å¯¹åŸå§‹3ä¸ªç‰¹å¾è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œç”Ÿæˆ5ä¸ªç‰¹å¾
    
    Args:
        raw_data: numpy array [30, 3] - (breath_line, heart_line, signal_intensity)
    
    Returns:
        numpy array [30, 5] - ç‰¹å¾å·¥ç¨‹åçš„5ä¸ªç‰¹å¾
    """
    import pandas as pd
    
    # è½¬æ¢ä¸ºDataFrameæ–¹ä¾¿å¤„ç†
    df = pd.DataFrame(raw_data, columns=['breath_line', 'heart_line', 'signal_intensity'])
    
    # ç‰¹å¾å·¥ç¨‹ (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
    df['heart_rate'] = df['heart_line'].rolling(10, min_periods=1).std() * 100 + 70
    df['resp_rate'] = df['breath_line'].rolling(10, min_periods=1).std() * 50 + 15
    df['signal_quality'] = df['signal_intensity'] / 50.0
    
    # 5ä¸ªç‰¹å¾çš„é¡ºåºï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    features = ['resp_rate', 'heart_rate', 'breath_line', 'heart_line', 'signal_quality']
    
    # å¤„ç†ç¼ºå¤±å€¼
    df[features] = df[features].ffill().fillna(0)
    
    return df[features].values


# ä½¿ç”¨ç¤ºä¾‹ - é€‚é…ä½ çš„æ•°æ®
if __name__ == "__main__":
    # ä½ çš„CSVæ–‡ä»¶è·¯å¾„
    csv_file = "/work/ai/WHOAMI/device_info_13D2F34920008071211195A907_20250623_classifier_LABEL.csv"
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ3ç±»ç¡çœ åˆ†æœŸæ¨¡å‹...")
    print("="*50)
    
    # # å¿«é€Ÿæµ‹è¯•é€‰é¡¹ - å¦‚æœæ•°æ®å¤ªå¤§ï¼Œå¯ä»¥å…ˆç”¨å°æ ·æœ¬æµ‹è¯•
    USE_QUICK_TEST = False  # æ”¹ä¸ºFalseä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ŒTrueä½¿ç”¨éƒ¨åˆ†æ•°æ®æµ‹è¯•
    max_samples = 50000 if USE_QUICK_TEST else None  # å¿«é€Ÿæµ‹è¯•ä½¿ç”¨5ä¸‡è¡Œæ•°æ®
    
    if USE_QUICK_TEST:
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ - ä½¿ç”¨éƒ¨åˆ†æ•°æ®")
    else:
        print("ğŸŒ å®Œæ•´è®­ç»ƒæ¨¡å¼ - ä½¿ç”¨å…¨éƒ¨æ•°æ®")
    
    try:
        # è®­ç»ƒæ¨¡å‹
        model = train_simple_model(csv_file, epochs=150, max_samples=max_samples)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        print("\nğŸ”® å¼€å§‹å®æ—¶é¢„æµ‹...")
        print("="*50)
        
        # é¢„æµ‹é€‰é¡¹
        print("é€‰æ‹©é¢„æµ‹æ¨¡å¼:")
        print("1. æ¯ç§’é¢„æµ‹ - é¢„æµ‹100ç§’")
        print("2. æ¯10ç§’é¢„æµ‹ - é¢„æµ‹1000ç§’") 
        print("3. å¿«é€Ÿé¢„æµ‹ - é¢„æµ‹æ‰€æœ‰å¯èƒ½çš„æ ·æœ¬")
        
        # ä¸åŒé¢„æµ‹æ¨¡å¼
        # æ¨¡å¼1: æ¯ç§’æ˜¾ç¤ºï¼Œé¢„æµ‹100ç§’
        print("\nğŸ“Š æ¨¡å¼1: æ¯ç§’é¢„æµ‹ç»“æœ")
        predictions_1s = predict_realtime('simple_sleep_model.pth', csv_file, 
                                        max_predict_samples=100, print_interval=1)
        
        print("\nğŸ“Š æ¨¡å¼2: æ¯10ç§’é¢„æµ‹ç»“æœ")  
        predictions_10s = predict_realtime('simple_sleep_model.pth', csv_file,
                                         max_predict_samples=1000, print_interval=10)
        
        print(f"âœ… é¢„æµ‹å®Œæˆ")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        with open('sleep_predictions_1s.json', 'w', encoding='utf-8') as f:
            json.dump(predictions_1s, f, ensure_ascii=False, indent=2)
        
        with open('sleep_predictions_10s.json', 'w', encoding='utf-8') as f:
            json.dump(predictions_10s, f, ensure_ascii=False, indent=2)
            
        print("ğŸ“„ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: sleep_predictions_1s.json å’Œ sleep_predictions_10s.json")
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°CSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ï¼")
        print("å½“å‰æŸ¥æ‰¾æ–‡ä»¶:", csv_file)
        print("è¯·å°†ä½ çš„CSVæ–‡ä»¶æ”¾åœ¨è„šæœ¬åŒç›®å½•ä¸‹ï¼Œæˆ–ä¿®æ”¹csv_fileå˜é‡")

    except Exception as e:
        print(f"âŒ å‡ºé”™äº†: {e}")
        import traceback
        traceback.print_exc()
        print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("ç¡®ä¿CSVåŒ…å«åˆ—: create_time, breath_line, heart_line, distance, signal_intensity, label")
        
        
    """
    # predict
    import numpy as np

    # ç”Ÿæˆ30ç§’ Ã— 3ä¸ªç‰¹å¾çš„æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
    sample_data = np.random.randn(30, 3)

    # æˆ–è€…ç”Ÿæˆæ›´çœŸå®çš„ä¼ æ„Ÿå™¨æ•°æ®èŒƒå›´
    sample_data = np.array([
        np.random.normal(0, 1, 30),      # breath_line: å‘¼å¸ä¿¡å· (å‡å€¼0, æ ‡å‡†å·®1)
        np.random.normal(0, 1, 30),      # heart_line: å¿ƒç”µä¿¡å· (å‡å€¼0, æ ‡å‡†å·®1)  
        np.random.uniform(20, 80, 30)    # signal_intensity: ä¿¡å·å¼ºåº¦ (20-80èŒƒå›´)
    ]).T  # è½¬ç½®ä¸º (30, 3)
    scaler = load_sleep_scaler(scaler_path="/work/ai/WHOAMI/sleep_scaler.pkl")
    model = SimpleSleepNet(
        input_size=5,
        seq_length=30, 
        num_classes=3,
    )
    
    # checkpoint = torch.load(self.model_path, map_location=self.device)
    model.load_state_dict(torch.load("/work/ai/WHOAMI/simple_sleep_model.pth"))
    print(sample_data)
    result, confidence = inference_single_sample_(sample_data=sample_data, model=model, scaler=scaler, use_raw_features=True)
    print(result, confidence)
    """