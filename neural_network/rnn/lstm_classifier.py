#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time Â  Â : 2025/06/26 10:28
@Author Â : weiyutao
@File Â  Â : lstm_classify.py
"""


import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
import numpy as np
import pandas as pd
import time
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")


class GRUClassifier(nn.Module):
    """
    - è½»é‡çº§GRUæ—¶åºåˆ†ç±»å™¨
    - å•å±‚GRUï¼ˆæ›¿ä»£åŒå±‚LSTMï¼‰
    - embedding_dim=64ï¼ˆå‡å°‘å‚æ•°ï¼‰
    - ç®€å•åˆ†ç±»å¤´è®¾è®¡
    """
    def __init__(
        self, 
        seq_len, 
        n_features, 
        n_classes,
        embedding_dim=64,
        dropout=0.2
    ):
        super(GRUClassifier, self).__init__()
        
        self.seq_len = seq_len
        self.n_features = n_features
        self.n_classes = n_classes
        self.embedding_dim = embedding_dim
        
        # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›ï¼šå•å±‚GRUæ›¿ä»£åŒå±‚LSTM
        self.gru = nn.GRU(
            input_size=n_features,
            hidden_size=embedding_dim,
            num_layers=1,  # å•å±‚
            batch_first=True,
            dropout=0,  # å•å±‚ä¸éœ€è¦dropout
            bidirectional=False  # å®æ—¶æ¨ç†ä¸ç”¨åŒå‘
        )
        
        # ğŸ¯ åˆ†ç±»å¤´ï¼šLinear -> Dropout -> Linear
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embedding_dim, n_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for name, param in self.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
            elif 'classifier' in name and 'weight' in name:
                nn.init.xavier_uniform_(param.data)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: (batch_size, seq_len, n_features)
        Returns:
            output: (batch_size, n_classes)
        """
        # GRUç‰¹å¾æå–
        gru_out, hidden = self.gru(x)  # gru_out: (batch, seq_len, embedding_dim)
        
        # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = gru_out[:, -1, :]  # (batch, embedding_dim)
        
        # åˆ†ç±»é¢„æµ‹
        logits = self.classifier(last_output)  # (batch, n_classes)
        
        return logits
    
    def get_feature_embeddings(self, x):
        """è·å–ç‰¹å¾åµŒå…¥ï¼ˆç”¨äºåˆ†æï¼‰"""
        with torch.no_grad():
            gru_out, _ = self.gru(x)
            return gru_out[:, -1, :].cpu().numpy()


def create_classification_sequences(data, labels, seq_length, slide_window_flag=1):
    """
    åˆ›å»ºåˆ†ç±»ä»»åŠ¡çš„åºåˆ—æ•°æ®
    
    Args:
        data: åŸå§‹æ•°æ® (n_samples, n_features)
        labels: æ ‡ç­¾ (n_samples,)
        seq_length: åºåˆ—é•¿åº¦
        slide_window_flag: æ˜¯å¦ä½¿ç”¨æ»‘åŠ¨çª—å£
    
    Returns:
        sequences: (n_sequences, seq_length, n_features)
        sequence_labels: (n_sequences,)
    """
    data = np.array(data)
    labels = np.array(labels)
    
    if slide_window_flag:
        # æ»‘åŠ¨çª—å£ï¼šæ¯ä¸ªçª—å£ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´ç‚¹çš„æ ‡ç­¾
        sequences = []
        sequence_labels = []
        
        for i in range(len(data) - seq_length + 1):
            sequences.append(data[i:i + seq_length])
            sequence_labels.append(labels[i + seq_length - 1])  # ä½¿ç”¨çª—å£æœ€åä¸€ä¸ªæ ‡ç­¾
            
        return np.array(sequences), np.array(sequence_labels)
    else:
        # éæ»‘åŠ¨çª—å£ï¼šç›´æ¥åˆ†å‰²
        n_sequences = len(data) // seq_length
        n_samples_needed = n_sequences * seq_length
        
        data_reshaped = data[:n_samples_needed].reshape(n_sequences, seq_length, -1)
        # å¯¹äºæ ‡ç­¾ï¼Œä½¿ç”¨æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæ ‡ç­¾
        labels_reshaped = labels[:n_samples_needed].reshape(n_sequences, seq_length)
        sequence_labels = labels_reshaped[:, -1]  # æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæ ‡ç­¾
        
        return data_reshaped, sequence_labels


def create_classification_dataloaders(
    train_data, train_labels, 
    val_data, val_labels, 
    batch_size=64
):
    """åˆ›å»ºåˆ†ç±»ä»»åŠ¡çš„DataLoader"""
    
    # è½¬æ¢ä¸ºtensor
    train_data = torch.FloatTensor(train_data)
    train_labels = torch.LongTensor(train_labels)
    val_data = torch.FloatTensor(val_data)
    val_labels = torch.LongTensor(val_labels)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TensorDataset(train_data, train_labels)
    val_dataset = TensorDataset(val_data, val_labels)
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader



def train_classifier(
    model, 
    train_loader, 
    val_loader, 
    n_epochs=100,
    lr=1e-3,
    weight_decay=1e-5,
    patience=15,
    save_checkpoints=True
):
    """
    è®­ç»ƒåˆ†ç±»å™¨
    """
    # ğŸ”§ ä¼˜åŒ–å™¨é…ç½®
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=lr,
        weight_decay=weight_decay  # è½»å¾®æƒé‡è¡°å‡
    )
    
    # ğŸ¯ åˆ†ç±»æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss().to(device)
    
    # ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, 
        patience=patience//3, verbose=True
    )
    
    # ğŸ“Š è®°å½•è®­ç»ƒå†å²
    history = {
        'train_loss': [], 'val_loss': [],
        'train_acc': [], 'val_acc': [],
        'lr': []
    }
    
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    patience_counter = 0
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒGRUåˆ†ç±»å™¨")
    print(f"ğŸ“‹ é…ç½®: epochs={n_epochs}, lr={lr}, patience={patience}")
    print("=" * 80)
    
    for epoch in range(1, n_epochs + 1):
        # =================== è®­ç»ƒé˜¶æ®µ ===================
        model.train()
        train_losses = []
        train_predictions = []
        train_targets = []
        
        for batch_idx, (batch_data, batch_labels) in enumerate(train_loader):
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            logits = model(batch_data)
            loss = criterion(logits, batch_labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # è®°å½•
            train_losses.append(loss.item())
            predictions = torch.argmax(logits, dim=1)
            train_predictions.extend(predictions.cpu().numpy())
            train_targets.extend(batch_labels.cpu().numpy())
            
            # è¿›åº¦æ˜¾ç¤º
            if batch_idx % 50 == 0:
                print(f"\rEpoch {epoch} [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}", end='')
        
        # =================== éªŒè¯é˜¶æ®µ ===================
        model.eval()
        val_losses = []
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for batch_data, batch_labels in val_loader:
                batch_data = batch_data.to(device)
                batch_labels = batch_labels.to(device)
                
                logits = model(batch_data)
                loss = criterion(logits, batch_labels)
                
                val_losses.append(loss.item())
                predictions = torch.argmax(logits, dim=1)
                val_predictions.extend(predictions.cpu().numpy())
                val_targets.extend(batch_labels.cpu().numpy())
        
        # =================== è®¡ç®—æŒ‡æ ‡ ===================
        train_loss = np.mean(train_losses)
        val_loss = np.mean(val_losses)
        train_acc = accuracy_score(train_targets, train_predictions)
        val_acc = accuracy_score(val_targets, val_predictions)
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        history['lr'].append(current_lr)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            best_model_wts = copy.deepcopy(model.state_dict())
            patience_counter = 0
            improvement = " â­ NEW BEST!"
            
            if save_checkpoints:
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_acc': best_acc,
                    'history': history
                }, f'best_gru_classifier_epoch_{epoch}.pth')
        else:
            patience_counter += 1
            improvement = ""
        
        # æ‰“å°è¿›åº¦
        print(f'\nEpoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} '
              f'train_acc={train_acc:.4f} val_acc={val_acc:.4f}{improvement}')
        
        # ğŸ›‘ æ—©åœ
        if patience_counter >= patience:
            print(f"\nğŸ›‘ Early stopping at epoch {epoch} (patience: {patience})")
            break
    
    # åŠ è½½æœ€ä½³æƒé‡
    model.load_state_dict(best_model_wts)
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")
    
    return model.eval(), history


def evaluate_classifier(model, test_loader, class_names=None):
    """
    è¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½
    """
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_data, batch_labels in test_loader:
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)
            
            logits = model(batch_data)
            predictions = torch.argmax(logits, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(batch_labels.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_predictions)
    
    print(f"\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_targets, all_predictions, target_names=class_names))
    
    return accuracy, all_predictions, all_targets


def inference_single_sample(model, sample, scaler=None):
    """
    å•æ ·æœ¬æ¨ç†
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        sample: (seq_len, n_features) å•ä¸ªåºåˆ—
        scaler: å½’ä¸€åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        prediction: é¢„æµ‹ç±»åˆ«
        confidence: é¢„æµ‹ç½®ä¿¡åº¦
    """
    model.eval()
    try:
        # é¢„å¤„ç†
        if scaler is not None:
            sample_reshaped = sample.reshape(-1, sample.shape[-1])
            sample_normalized = scaler.transform(sample_reshaped)
            sample = sample_normalized.reshape(sample.shape)
        
        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
        sample_tensor = torch.FloatTensor(sample).unsqueeze(0).to(device)
        with torch.no_grad():
            logits = model(sample_tensor)
            probabilities = F.softmax(logits, dim=1)
            prediction = torch.argmax(logits, dim=1).item()
            confidence = torch.max(probabilities).item()
            print(f"prediction: {prediction}")
            print(f"confidence: {confidence}")
    except Exception as e:
        raise ValueError("fail to exec lstm classifier model! {e}") from e
    return prediction, confidence



def write_data_to_file(sequences, labels, filename):
    """
    å°†åºåˆ—æ•°æ®å’Œæ ‡ç­¾å†™å…¥æ–‡ä»¶ï¼Œæ ¼å¼ä¸ºLSTM-FCNè¦æ±‚çš„æ ¼å¼
    
    å‚æ•°:
    sequences: numpy array, å½¢çŠ¶ä¸º (n_samples, sequence_length) æˆ– (n_samples, sequence_length, n_features)
    labels: numpy array, å½¢çŠ¶ä¸º (n_samples,)ï¼ŒåŒ…å«ç±»åˆ«æ ‡ç­¾
    filename: str, è¾“å‡ºæ–‡ä»¶å
    """
    
    # ç¡®ä¿sequencesæ˜¯2Dçš„
    if len(sequences.shape) == 3:
        # å¦‚æœæ˜¯3Dæ•°ç»„ä¸”æœ€åä¸€ç»´æ˜¯1ï¼Œåˆ™å‹ç¼©æ‰
        if sequences.shape[-1] == 1:
            sequences = sequences.squeeze(-1)
        else:
            # å¦‚æœæœ‰å¤šä¸ªç‰¹å¾ï¼Œéœ€è¦å±•å¹³æˆ–é€‰æ‹©ä¸€ä¸ªç‰¹å¾
            print(f"è­¦å‘Šï¼šæ£€æµ‹åˆ°å¤šç‰¹å¾æ•°æ® {sequences.shape}ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾")
            sequences = sequences[:, :, 0]
    
    # æ‰“å¼€æ–‡ä»¶è¿›è¡Œå†™å…¥
    with open(filename, 'w') as f:
        for i in range(len(sequences)):
            # è·å–æ ‡ç­¾ï¼ˆç¡®ä¿æ˜¯æ•´æ•°ï¼‰
            label = int(labels[i])
            
            # è·å–æ—¶é—´åºåˆ—æ•°æ®
            time_series = sequences[i]
            
            # æ„å»ºè¡Œï¼šæ ‡ç­¾ + æ—¶é—´åºåˆ—å€¼
            line_parts = [str(label)]
            line_parts.extend([str(value) for value in time_series])
            
            # ç”¨ç©ºæ ¼è¿æ¥å¹¶å†™å…¥æ–‡ä»¶
            line = ' '.join(line_parts)
            f.write(line + '\n')
    
    print(f"å·²æˆåŠŸå†™å…¥ {len(sequences)} ä¸ªæ ·æœ¬åˆ° {filename}")
    print(f"æ¯ä¸ªæ ·æœ¬é•¿åº¦: {sequences.shape[1]}")
    print(f"æ ‡ç­¾èŒƒå›´: {np.min(labels)} - {np.max(labels)}")




# =================== ç¤ºä¾‹ä½¿ç”¨ ===================
if __name__ == '__main__':
    # ğŸ”§ å‚æ•°é…ç½®
    seq_len = 60
    n_features = 2
    n_classes = 3  # å‡è®¾3åˆ†ç±»ä»»åŠ¡
    batch_size = 64
    
    print("ğŸ”¥ è½»é‡çº§GRUæ—¶åºåˆ†ç±»å™¨ - ç¬¬ä¸€é˜¶æ®µå®ç°")
    print(f"ğŸ“‹ é…ç½®: seq_len={seq_len}, n_features={n_features}, n_classes={n_classes}")
    
    # ğŸš€ åˆ›å»ºæ¨¡å‹
    model = GRUClassifier(
        seq_len=seq_len,
        n_features=n_features, 
        n_classes=n_classes,
        embedding_dim=64,  # ğŸ¯ å‡å°‘åˆ°64
        dropout=0.5
    ).to(device)
    
    # ğŸ“Š æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"ğŸ“ˆ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"ğŸ“ æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")
    print("\nâœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼å‡†å¤‡å¼€å§‹è®­ç»ƒ...")
    
    
    original_data = pd.read_csv("/work/ai/WHOAMI/device_info_13D2F34920008071211195A907_20250623_classifier_LABEL.csv", encoding='gbk')
    original_data = np.array(original_data)
    train_data = original_data[:, 1:-3]
    train_label = np.array([{'æ¸…é†’': 0, 'æµ…ç¡çœ ': 1, 'æ·±ç¡çœ ': 2}[label] for label in original_data[:, -1]])
    train_sequences, train_seq_labels = create_classification_sequences(
        train_data, train_label, seq_len, slide_window_flag=1
    )
    train_sequences = np.array(train_sequences, dtype=np.float32)
    train_labels = np.array(train_seq_labels, dtype=np.int64)
    print(len(original_data))
    print(len(train_sequences))
    print(len(train_seq_labels))
    print(train_sequences)
    print(train_seq_labels)
    
    # # æ•°æ®åˆ†å‰²
    split_idx = int(len(train_sequences) * 0.8)
    val_sequences = train_sequences[split_idx:]
    val_seq_labels = train_seq_labels[split_idx:]
    train_sequences = train_sequences[:split_idx]
    train_seq_labels = train_seq_labels[:split_idx]
    
    
    # å†™å…¥è®­ç»ƒæ•°æ®
    write_data_to_file(train_sequences, train_seq_labels, 'train_data.txt')

    # å†™å…¥æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†ï¼‰
    write_data_to_file(val_sequences, val_seq_labels, 'test.txt')
    
    
    # åˆ›å»ºDataLoader
    # train_loader, val_loader = create_classification_dataloaders(
    #     train_sequences, train_seq_labels,
    #     val_sequences, val_seq_labels,
    #     batch_size=batch_size
    # )
    
    # # ğŸš€ è®­ç»ƒæ¨¡å‹
    # model, history = train_classifier(
    #     model, train_loader, val_loader,
    #     n_epochs=50, lr=5e-4
    # )
    
    # # ğŸ“Š è¯„ä¼°æ¨¡å‹
    # accuracy, predictions, targets = evaluate_classifier(model, val_loader)
    
    
    # model = GRUClassifier(
    #     seq_len=seq_len,
    #     n_features=n_features, 
    #     n_classes=n_classes,
    #     embedding_dim=64,  # ğŸ¯ å‡å°‘åˆ°64
    #     dropout=0.2
    # ).to(device)
    # checkpoint = torch.load("/work/ai/WHOAMI/whoami/neural_network/best_gru_classifier_epoch_6_classifier_2dimensions.pth", map_location=device)
    # model.load_state_dict(checkpoint['model_state_dict'])
    # for i in range(5):
    #     print(train_sequences[i])
    #     print(type(train_sequences[i]))
    #     result, confidence = inference_single_sample(model, sample=train_sequences[i])
    #     print(result, confidence)