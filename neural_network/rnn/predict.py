#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time Â  Â : 2025/06/20 15:47
@Author Â : weiyutao
@File Â  Â : predict.py
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
import warnings
warnings.filterwarnings('ignore')

from auto_encoders import *

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def detect_anomalies(reconstruction_errors, threshold_method='percentile', threshold_value=95):
    """
    åŸºäºé‡æ„è¯¯å·®æ£€æµ‹å¼‚å¸¸
    
    Args:
        reconstruction_errors: é‡æ„è¯¯å·®æ•°ç»„
        threshold_method: é˜ˆå€¼æ–¹æ³• ('percentile', 'std', 'fixed')
        threshold_value: é˜ˆå€¼å‚æ•°
    
    Returns:
        anomalies: å¼‚å¸¸æ ‡è®° (bool array)
        threshold: ä½¿ç”¨çš„é˜ˆå€¼
    """
    if threshold_method == 'percentile':
        threshold = np.percentile(reconstruction_errors, threshold_value)
    elif threshold_method == 'std':
        mean_error = np.mean(reconstruction_errors)
        std_error = np.std(reconstruction_errors)
        threshold = mean_error + threshold_value * std_error
    elif threshold_method == 'fixed':
        threshold = threshold_value
    else:
        raise ValueError("threshold_method must be 'percentile', 'std', or 'fixed'")
    
    anomalies = reconstruction_errors > threshold
    
    return anomalies, threshold


def visualize_results(reconstruction_errors, anomalies, threshold, raw_data=None, feature_names=None):
    """
    å¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ
    
    Args:
        reconstruction_errors: é‡æ„è¯¯å·®
        anomalies: å¼‚å¸¸æ ‡è®°
        threshold: é˜ˆå€¼
        raw_data: åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
        feature_names: ç‰¹å¾åç§°ï¼ˆå¯é€‰ï¼‰
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. é‡æ„è¯¯å·®æ—¶é—´åºåˆ—
    axes[0, 0].plot(reconstruction_errors, alpha=0.7, label='é‡æ„è¯¯å·®')
    axes[0, 0].axhline(y=threshold, color='r', linestyle='--', label=f'é˜ˆå€¼: {threshold:.4f}')
    axes[0, 0].scatter(np.where(anomalies)[0], reconstruction_errors[anomalies], 
                      color='red', s=20, label=f'å¼‚å¸¸ç‚¹ ({np.sum(anomalies)}ä¸ª)')
    axes[0, 0].set_title('é‡æ„è¯¯å·®æ—¶é—´åºåˆ—')
    axes[0, 0].set_xlabel('æ—¶é—´æ­¥')
    axes[0, 0].set_ylabel('é‡æ„è¯¯å·®')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. é‡æ„è¯¯å·®åˆ†å¸ƒ
    axes[0, 1].hist(reconstruction_errors, bins=50, alpha=0.7, color='skyblue', density=True)
    axes[0, 1].axvline(x=threshold, color='r', linestyle='--', label=f'é˜ˆå€¼: {threshold:.4f}')
    axes[0, 1].set_title('é‡æ„è¯¯å·®åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('é‡æ„è¯¯å·®')
    axes[0, 1].set_ylabel('å¯†åº¦')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. å¼‚å¸¸ç‚¹åœ¨æ—¶é—´è½´ä¸Šçš„åˆ†å¸ƒ
    anomaly_indices = np.where(anomalies)[0]
    axes[1, 0].scatter(anomaly_indices, np.ones_like(anomaly_indices), 
                      color='red', alpha=0.6, s=30)
    axes[1, 0].set_title(f'å¼‚å¸¸ç‚¹æ—¶é—´åˆ†å¸ƒ (æ€»è®¡: {len(anomaly_indices)}ä¸ª)')
    axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
    axes[1, 0].set_yticks([])
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. ç»Ÿè®¡ä¿¡æ¯
    stats_text = f"""
    æ•°æ®ç»Ÿè®¡:
    â€¢ æ€»åºåˆ—æ•°: {len(reconstruction_errors)}
    â€¢ å¼‚å¸¸åºåˆ—æ•°: {np.sum(anomalies)}
    â€¢ å¼‚å¸¸ç‡: {np.sum(anomalies)/len(reconstruction_errors)*100:.2f}%
    
    è¯¯å·®ç»Ÿè®¡:
    â€¢ å¹³å‡è¯¯å·®: {np.mean(reconstruction_errors):.4f}
    â€¢ è¯¯å·®æ ‡å‡†å·®: {np.std(reconstruction_errors):.4f}
    â€¢ æœ€å¤§è¯¯å·®: {np.max(reconstruction_errors):.4f}
    â€¢ æœ€å°è¯¯å·®: {np.min(reconstruction_errors):.4f}
    """
    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, 
                    fontsize=11, verticalalignment='center',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    axes[1, 1].set_xlim(0, 1)
    axes[1, 1].set_ylim(0, 1)
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    return fig


def save_results(reconstruction_errors, anomalies, threshold, output_path='anomaly_results.csv'):
    """
    ä¿å­˜å¼‚å¸¸æ£€æµ‹ç»“æœ
    
    Args:
        reconstruction_errors: é‡æ„è¯¯å·®
        anomalies: å¼‚å¸¸æ ‡è®°
        threshold: é˜ˆå€¼
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    results_df = pd.DataFrame({
        'sequence_id': range(len(reconstruction_errors)),
        'reconstruction_error': reconstruction_errors,
        'is_anomaly': anomalies,
        'threshold': threshold
    })
    
    results_df.to_csv(output_path, index=False)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return results_df


def main_inference_pipeline(
    model_path,
    data_path,
    scaler_path=None,
    seq_len=60,
    n_features=6,
    batch_size=32,
    threshold_method='std',
    threshold_value=2,
    output_path='anomaly_results.csv',
    visualize=True
):
    """
    å®Œæ•´çš„æ¨ç†æµæ°´çº¿
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        scaler_path: å½’ä¸€åŒ–å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        seq_len: åºåˆ—é•¿åº¦
        n_features: ç‰¹å¾æ•°é‡
        batch_size: æ‰¹æ¬¡å¤§å°
        threshold_method: é˜ˆå€¼æ–¹æ³•
        threshold_value: é˜ˆå€¼å‚æ•°
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        visualize: æ˜¯å¦å¯è§†åŒ–
    
    Returns:
        results_df: ç»“æœDataFrame
    """
    print("ğŸš€ å¼€å§‹å¼‚å¸¸æ£€æµ‹æ¨ç†æµæ°´çº¿...")
    print("=" * 50)
    
    # 1. åŠ è½½æ¨¡å‹å’Œå½’ä¸€åŒ–å™¨
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    model, scaler = load_model_and_scaler(model_path, scaler_path)
    
    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    print("ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®...")
    if data_path.endswith('.csv'):
        test_data = pd.read_csv(data_path)
    else:
        test_data = np.load(data_path)
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
    
    # 3. æ•°æ®é¢„å¤„ç†
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
    tensor_data, scaler, original_shape = preprocess_inference_data(
        test_data, seq_len, n_features, scaler, slide_window_flag=1
    )
    
    # 4. åˆ›å»ºDataLoader
    dataset = TensorDataset(tensor_data, tensor_data)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    # 5. æ‰¹æ¬¡æ¨ç†
    print("ğŸ”® æ‰§è¡Œæ¨ç†...")
    reconstructed_data, reconstruction_errors, raw_errors = batch_inference(
        model, dataloader, device, return_errors=True
    )
    
    # 6. å¼‚å¸¸æ£€æµ‹
    print("ğŸ” æ£€æµ‹å¼‚å¸¸...")
    anomalies, threshold = detect_anomalies(
        reconstruction_errors, threshold_method, threshold_value
    )
    
    print(f"æ£€æµ‹åˆ° {np.sum(anomalies)} ä¸ªå¼‚å¸¸åºåˆ— (å¼‚å¸¸ç‡: {np.sum(anomalies)/len(anomalies)*100:.2f}%)")
    
    # 7. å¯è§†åŒ–ç»“æœ
    if visualize:
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
        visualize_results(reconstruction_errors, anomalies, threshold)
    
    # 8. ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    results_df = save_results(reconstruction_errors, anomalies, threshold, output_path)
    
    print("âœ… æ¨ç†å®Œæˆ!")
    return results_df


def create_training_scaler():
    """
    é‡æ–°åˆ›å»ºè®­ç»ƒæ—¶ä½¿ç”¨çš„å½’ä¸€åŒ–å™¨
    ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„æ•°æ®å’Œæ­¥éª¤
    """
    print("ğŸ”§ åˆ›å»ºè®­ç»ƒæ—¶çš„å½’ä¸€åŒ–å™¨...")
    
    # 1. åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    train_data = pd.read_csv("/work/soft/LSTM-Autoencoders/kdd_data/device_info_20250616_nomaly.csv")
    train_data = np.array(train_data)
    
    # 2. åˆ›å»ºåºåˆ—ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    seq_len = 60
    n_features = 7
    slide_window_flag = 1
    
    if slide_window_flag:
        sequences = []
        for i in range(len(train_data) - seq_len + 1):
            sequences.append(train_data[i:i + seq_len])
        train_data = np.array(sequences)
    
    # 3. åˆ†å‰²æ•°æ®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    train_ratio = 0.8
    n_samples = len(train_data)
    split_idx = int(n_samples * train_ratio)
    train_data = train_data[:split_idx]  # åªè¦è®­ç»ƒéƒ¨åˆ†
    
    # 4. åˆ›å»ºå½’ä¸€åŒ–å™¨ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    train_shape = train_data.shape
    train_reshaped = train_data.reshape(-1, train_shape[-1])  # (n_samples * seq_len, n_features)
    
    scaler = StandardScaler()  # ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹æ³•
    scaler.fit(train_reshaped)
    
    print(f"âœ… å½’ä¸€åŒ–å™¨åˆ›å»ºæˆåŠŸ!")
    print(f"   ç‰¹å¾æ•°é‡: {scaler.n_features_in_}")
    print(f"   ç‰¹å¾å‡å€¼: {scaler.mean_}")
    print(f"   ç‰¹å¾æ ‡å‡†å·®: {scaler.scale_}")
    
    return scaler


def load_model_and_scaler(model_path, scaler_path=None):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå½’ä¸€åŒ–å™¨
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        scaler_path: å½’ä¸€åŒ–å™¨æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    
    Returns:
        model, scaler
    """
    # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location=device)
    
    # é‡å»ºæ¨¡å‹ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶å‚æ•°ä¸€è‡´ï¼‰
    seq_len = 60  # æ ¹æ®ä½ çš„è®­ç»ƒå‚æ•°è°ƒæ•´
    n_features = 6
    embedding_dim = 128
    
    model = RecurrentAutoencoder(seq_len, n_features, embedding_dim)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! æœ€ä½³æŸå¤±: {checkpoint['best_loss']:.6f}")
    
    # å¦‚æœæœ‰å•ç‹¬çš„scaleræ–‡ä»¶ï¼ŒåŠ è½½å®ƒ
    scaler = None
    if scaler_path:
        import joblib
        scaler = joblib.load(scaler_path)
        print("âœ… å½’ä¸€åŒ–å™¨åŠ è½½æˆåŠŸ!")
    
    return model, scaler

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    MODEL_PATH = "/work/ai/WHOAMI/whoami/neural_network/rnn/checkpoint_epoch_147.pth"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    # DATA_PATH = "/work/soft/LSTM-Autoencoders/kdd_data/device_info_20250616_nomaly.csv"        # æ›¿æ¢ä¸ºä½ çš„æµ‹è¯•æ•°æ®è·¯å¾„
    DATA_PATH = "/work/ai/WHOAMI/whoami/neural_network/rnn/device_info_20250616.csv"        # æ›¿æ¢ä¸ºä½ çš„æµ‹è¯•æ•°æ®è·¯å¾„
    SCALER_PATH = None                      # å¦‚æœæœ‰å•ç‹¬çš„scaleræ–‡ä»¶
    
    """
    å¦‚æœé¢„æµ‹æ•°æ®å¾ˆå°‘ä¸”é¢„æµ‹æ•°æ®å’Œè®­ç»ƒæ•°æ®åˆ†å¸ƒåŸºæœ¬ä¸€è‡´ï¼Œå¯ä»¥ä½¿ç”¨è®­ç»ƒæ•°æ®çš„scalerå»è¿›è¡Œå½’ä¸€åŒ–
    å› ä¸ºè®­ç»ƒæ•°æ®å’Œé¢„æµ‹æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾åŸºæœ¬ç›¸ä¼¼ï¼Œä½†æ˜¯å¦‚æœåˆ†å¸ƒä¸åŒéœ€è¦ä½¿ç”¨é¢„æµ‹æ•°æ®çš„scaler
    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºè®­ç»ƒæ—¶çš„å½’ä¸€åŒ–å™¨
    training_scaler = create_training_scaler()
    
    # ç¬¬äºŒæ­¥ï¼šä¿å­˜å½’ä¸€åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
    import joblib
    joblib.dump(training_scaler, 'training_scaler.pkl')
    print("ğŸ“ å½’ä¸€åŒ–å™¨å·²ä¿å­˜åˆ°: training_scaler.pkl")
    """
    
    
    # æ‰§è¡Œæ¨ç†
    # scaler_path=Noneå³å¯åœ¨åˆå§‹åŒ–æ•°æ®çš„æ—¶å€™ä½¿ç”¨é¢„æµ‹æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾å»åˆ›å»ºå½’ä¸€åŒ–scaler
    results = main_inference_pipeline(
        model_path=MODEL_PATH,
        data_path=DATA_PATH,
        scaler_path='/work/ai/WHOAMI/whoami/neural_network/rnn/training_scaler.pkl',
        seq_len=60,
        n_features=6,
        batch_size=32,
        threshold_method='fixed',  # 'percentile', 'std', 'fixed'
        threshold_value=0.5,             # å¯¹äºpercentileæ˜¯ç™¾åˆ†ä½æ•°ï¼Œå¯¹äºstdæ˜¯æ ‡å‡†å·®å€æ•°
        output_path='anomaly_results.csv',
        visualize=True
    )
    
    # æŸ¥çœ‹ç»“æœæ‘˜è¦
    print("\nğŸ“‹ ç»“æœæ‘˜è¦:")
    print(results.describe())
    
    # æŸ¥çœ‹å¼‚å¸¸æ ·æœ¬
    anomaly_samples = results[results['is_anomaly'] == True]
    print(f"\nğŸš¨ å¼‚å¸¸æ ·æœ¬ (å‰10ä¸ª):")
    print(anomaly_samples.head(10))