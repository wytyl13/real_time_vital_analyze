# ğŸš€ æ¨¡å‹å¾®è°ƒã€é‡åŒ–ä¸éƒ¨ç½²å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—æ¶µç›–äº†ä»æ¨¡å‹å¾®è°ƒåˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ LoRA å¾®è°ƒã€GGUF é‡åŒ–ã€ä»¥åŠ Ollama éƒ¨ç½²ï¼Œä»…ä¸ºæ“ä½œï¼Œä¸åŒ…å«ç†è®ºè§£æã€‚

---

## ğŸ“‹ ç›®å½•

- [æ¨¡å‹å¾®è°ƒ](#-æ¨¡å‹å¾®è°ƒ)
- [æ¨¡å‹é‡åŒ–](#-æ¨¡å‹é‡åŒ–)  
- [Ollama éƒ¨ç½²](#-ollama-éƒ¨ç½²)
- [VLLM éƒ¨ç½²](#-vllm-éƒ¨ç½²)

---

## ğŸ”§ æ¨¡å‹å¾®è°ƒ

### è°ƒè¯•é…ç½®

é¦–å…ˆéœ€è¦åœ¨è®­ç»ƒå™¨ä¸­æ·»åŠ è°ƒè¯•ä»£ç æ¥æŸ¥çœ‹æ•°æ®é¢„å¤„ç†æ•ˆæœï¼š

**ç¼–è¾‘æ–‡ä»¶ï¼š** `src/llamafactory/train/sft/trainer.py`

åœ¨ä»¥ä¸‹ä»£ç å—åæ·»åŠ è°ƒè¯•é€»è¾‘ï¼š

```python
if is_transformers_version_greater_than("4.46"):
    kwargs["processing_class"] = kwargs.pop("tokenizer")
else:
    self.processing_class: PreTrainedTokenizer = kwargs.get("tokenizer")

super().__init__(**kwargs)

# âœ¨ æ·»åŠ è°ƒè¯•ä»£ç ï¼Œæ‰“å°è¾“å…¥tokensï¼Œè¾“å…¥æ¨¡å‹çš„å­—ç¬¦ä¸²å’Œlabels
import os
print("\n=== ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹é¢„å¤„ç†åçš„æ ·æœ¬ ===")
train_dataset = kwargs.get('train_dataset')
if train_dataset:
    for i in range(min(10, len(train_dataset))):
        sample = train_dataset[i]
        print(f"\nğŸ“ æ ·æœ¬ {i+1}:")
        print("ğŸ”¤ Input tokens:", sample['input_ids'][:100])
        print("ğŸ“„ Decoded text:", self.processing_class.batch_decode([sample['input_ids']], skip_special_tokens=False)[0])
        if 'labels' in sample:
            print("ğŸ·ï¸ Labels:", sample['labels'])
print("=== âœ… è°ƒè¯•ç»“æŸ ===\n")
```

### æ•°æ®é¢„å¤„ç†è°ƒè¯•

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ•°æ®é¢„å¤„ç†è°ƒè¯•ï¼š

```bash
**ç¼–è¾‘æ–‡ä»¶å¹¶æ–°å¢è®­ç»ƒä»»åŠ¡glaive_toolcall_en_demoæŒ‡å®šå¯¹åº”çš„è®­ç»ƒæ•°æ®é›†è·¯å¾„å’Œæ ¼å¼æ¨¡æ¿ç­‰ï¼š** `src/data/dataset_info.json`
export MODEL_PATH=/root/autodl-tmp/model/Qwen/Qwen2.5-7B-Instruct

python src/train.py \
    --stage sft \
    --model_name_or_path $MODEL_PATH \
    --dataset glaive_toolcall_en_demo \
    --template qwen \
    --cutoff_len 2048 \
    --do_train false
```

**è°ƒè¯•è¾“å‡ºç¤ºä¾‹ï¼š**

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹å®Œæ•´è°ƒè¯•è¾“å‡º</summary>

```
æ ·æœ¬ 1:
Input tokens: [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 382, 2, 13852, 271, 2610, 1231, 1618, 825, 476, 803, 5746, 311, 7789, 448, 279, 1196, 3239, 382, 2610, 525, 3897, 448, 729, 32628, 2878, 366, 15918, 1472, 15918, 29, 11874, 9492, 510, 27, 15918, 397, 4913, 1313, 788, 330, 1688, 497, 330, 1688, 788, 5212, 606, 788, 330, 1836, 7080, 8923, 497, 330, 4684, 788, 330, 5890, 369, 18627, 3118, 389, 13966, 497, 330, 13786, 788, 5212, 1313, 788, 330, 1700, 497, 330, 13193, 788, 5212, 38120, 788, 5212, 1313, 788]

Decoded text: <|im_start|>system
# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"type": "function", "function": {"name": "search_recipes", "description": "Search for recipes based on ingredients", "parameters": {"type": "object", "properties": {"ingredients": {"type": "array", "items": {"type": "string"}, "description": "The ingredients to search for"}}, "required": ["ingredients"]}}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>
<|im_start|>user
Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?<|im_end|>
<|im_start|>assistant
Of course! I can help you with that. Please tell me what ingredients you have.<|im_end|>
<|im_start|>user
I have chicken, bell peppers, and rice.<|im_end|>
<|im_start|>assistant
<tool_call>
{"name": "search_recipes", "arguments": {"ingredients": ["chicken", "bell peppers", "rice"]}}
</tool_call><|im_end|>

Labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2124, 3308, 0, 358, 646, 1492, 498, 448, 429, 13, 5209, 3291, 752, 1128, 13966, 498, 614, 13, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151657, 198, 4913, 606, 788, 330, 1836, 7080, 8923, 497, 330, 16370, 788, 5212, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 1341, 11248, 151658, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 1730, 1378, 18627, 369, 498, 13, 576, 1156, 825, 374, 330, 83863, 323, 17884, 51782, 64192, 52417, 3263, 576, 11221, 525, 25, 19649, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 13, 576, 2086, 825, 374, 330, 83863, 323, 29516, 25442, 261, 1263, 3263, 576, 11221, 525, 25, 12514, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 13, 15920, 825, 1035, 498, 1075, 311, 1430, 30, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 2776, 14589, 11, 714, 438, 458, 15235, 11, 358, 1513, 944, 614, 279, 22302, 311, 2736, 9250, 9079, 1741, 438, 21391, 13966, 13, 4354, 11, 358, 646, 1492, 498, 1477, 803, 18627, 476, 3410, 17233, 10414, 421, 498, 1184, 13, 151645, 198]
```
</details>

### è®­ç»ƒå‚æ•°é…ç½®

**å…³é”®è®­ç»ƒå‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | è®¡ç®—å…¬å¼ | ç¤ºä¾‹å€¼ |
|------|---------|--------|
| `warmup_steps` | çº¦ä¸ºæ€»æ­¥æ•°çš„ 20% | 20 |
| æœ‰æ•ˆæ‰¹æ¬¡å¤§å° | `batch_size Ã— gradient_accumulation_steps` | 2 Ã— 8 = 16 |
| æ€»æ­¥æ•° | `(samples Ã— epochs) Ã· æœ‰æ•ˆæ‰¹æ¬¡å¤§å°` | - |
| cutoff_len | æœ€å¤§è¾“å…¥é•¿åº¦ | 4096 |
| glaive_toolcall_en_demo | å¯¹åº”åœ¨data_info.jsonæ–‡ä»¶ä¸­çš„é…ç½®è®­ç»ƒåç§° | 4096 |

### æ‰§è¡Œå¾®è°ƒè®­ç»ƒ

```bash
python src/train.py \
    --stage sft \
    --do_train \
    --model_name_or_path $MODEL_PATH \
    --dataset glaive_toolcall_en_demo \
    --template qwen \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir /root/autodl-tmp/tool_call_full \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --cutoff_len 4096 \
    --num_train_epochs 5 \
    --warmup_steps 20 \
    --learning_rate 5e-6 \
    --logging_steps 10 \
    --save_steps 30 \
    --gradient_checkpointing true \
    --bf16
```

### æ¨¡å‹åˆå¹¶

å¾®è°ƒå®Œæˆåï¼Œéœ€è¦å°† LoRA é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli export \
    --model_name_or_path /root/autodl-tmp/model/Qwen/Qwen2.5-7B-Instruct \
    --adapter_name_or_path /root/autodl-tmp/tool_call_full \
    --template qwen \
    --finetuning_type lora \
    --export_dir /root/autodl-tmp/merge_model_tool_call_20250816_qwen \
    --export_size 2 \
    --export_legacy_format False
```
### æ¨¡å‹æµ‹è¯•
```python
import json
from transformers import AutoTokenizer, AutoModelForCausalLM

# å‡è®¾ä½ çš„æ¨¡å‹æ”¯æŒå·¥å…·è°ƒç”¨
model_path = "/root/autodl-tmp/merge_model_tool_call_20250816_qwen"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "calculator",
            "description": "è®¡ç®—æ•°å­¦è¡¨è¾¾å¼",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼"
                    }
                },
                "required": ["expression"]
            }
        }
    }
]


tools = [
    {
        "name": "HandleTongzhiTonggao",
        "description": "è¯¥å·¥å…·ä¸“ç”¨äºæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤æ—¶è®¯æ¶ˆæ¯æˆ–é€šå‘Šï¼",
        "parameters": [
        {
            "name": "operation",
            "description": "ç”¨æˆ·å¯¹æ—¶è®¯æ¶ˆæ¯æˆ–é€šå‘Šçš„æ“ä½œæ–¹å¼ï¼Œä»åˆ—è¡¨ä¸­é€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼š['ADD', 'DELETE', 'UPDATE', 'LIST']",
            "required": True
        },
        {
            "name": "type",
            "description": "ç”¨æˆ·ä¸Šä¼ æ¶ˆæ¯çš„ç±»å‹ï¼Œä»åˆ—è¡¨ä¸­é€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼š[\"æ—¶è®¯æ¶ˆæ¯\", \"é€šå‘Š\"]",
            "required": True
        },
        {
            "name": "content",
            "description": "ç”¨æˆ·ä¸Šä¼ æ¶ˆæ¯çš„å†…å®¹",
            "required": True
        }
        ]
    },
    {
        "name": "WeatherApi",
        "description": "è¯¥å·¥å…·ä¸“ç”¨äºå¸‚ã€åŒºå¿çš„å®æ—¶å¤©æ°”æŸ¥è¯¢",
        "parameters": [
        {
            "name": "query_key",
            "description": "éœ€è¦æŸ¥è¯¢å¤©æ°”çš„åœ°åŒºï¼ˆå¯ä»¥æ˜¯ç›´è¾–å¸‚ã€åŒºå¿ï¼‰",
            "required": True
        }
        ]
    },
    {
        "name": "WaterMachineApi",
        "description": "ç‰©è”ç½‘é¥®æ°´æœºæ“ä½œæ¥å£ã€‚",
        "parameters": [
        {
            "name": "operation",
            "description": "ä»ç”¨æˆ·å…³äºé¥®æ°´æœºæ“ä½œçš„é—®é¢˜ä¸­æå–ç›¸å…³çš„æ“ä½œæŒ‡ä»¤ï¼Œå¯é€‰æ‹©çš„æŒ‡ä»¤åŒ…å«ï¼š[æ°´å£¶å–æ°´, æ°´å£¶åœæ°´, åœæ­¢åŠ çƒ­, å¼€å§‹ä¿æ¸©, åœæ­¢ä¿æ¸©, æ‰“å¼€è¯­éŸ³, å…³é—­è¯­éŸ³]",
            "required": True
        }
        ]
    },
    {
        "name": "HealthReportSimple",
        "description": "ä¸“é—¨ç”¨äºå¤„ç†ç”¨æˆ·ä¸ªäººå¥åº·ç›‘æµ‹æ•°æ®çš„æ‰€æœ‰æŸ¥è¯¢ã€‚å½“ç”¨æˆ·é—®é¢˜æ˜ç¡®æ¶‰åŠä»¥ä¸‹ä»»ä¸€æ–¹é¢æ—¶ï¼Œç³»ç»Ÿå¿…é¡»ä¼˜å…ˆè°ƒç”¨æ­¤å·¥å…·ï¼š(1)ç¡çœ ç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚\"ç¡çœ æŠ¥å‘Š\"ã€\"ç¡çœ æ•°æ®\"ã€\"ç¡çœ è´¨é‡\"ã€\"ç¡çœ æ—¶é•¿\"ã€\"ç¡çœ æ•ˆç‡\"ã€\"ç¡çœ è¯„åˆ†\"ã€\"æ·±åº¦ç¡çœ \"ã€\"æµ…åº¦ç¡çœ \"ï¼‰ï¼›(2)ç”Ÿç†æŒ‡æ ‡ï¼ˆå¦‚\"å¿ƒç‡\"ã€\"å¿ƒç‡å¼‚å¸¸\"ã€\"å‘¼å¸ç‡\"ã€\"å‘¼å¸å¼‚å¸¸\"ã€\"ä½“åŠ¨æ¬¡æ•°\"ã€\"ä½“åŠ¨æŒ‡æ•°\"ï¼‰ï¼›(3)å¥åº·çŠ¶æ€ï¼ˆå¦‚\"å¥åº·è¯„åˆ†\"ã€\"å¥åº·çŠ¶å†µ\"ã€\"å¥åº·å¼‚å¸¸\"ï¼‰ï¼›(4)ä»»ä½•æ¶‰åŠç”¨æˆ·ä¸ªäººç›‘æµ‹æ•°æ®çš„æ—¶é—´æ€§æŸ¥è¯¢ï¼ˆå¦‚\"æ˜¨æ™š\"ã€\"æœ€è¿‘å‡ å¤©\"ã€\"æœ¬å‘¨\"ã€\"ä¸Šå‘¨\"ç­‰æ—¶é—´æ®µçš„å¥åº·æ•°æ®ï¼‰ã€‚HealthReportæä¾›ä¸“ä¸šçš„å¥åº·æ•°æ®è§£è¯»å’Œåˆ†æã€‚",
        "parameters": [
        {
            "name": "health_report_question",
            "description": "ç”¨æˆ·å…³äºç¡çœ å¥åº·æŠ¥å‘Šçš„å®Œæ•´é—®é¢˜ï¼Œéœ€æ ¹æ®å½“å‰é—®é¢˜å’Œå†å²å¯¹è¯ä¸Šä¸‹æ–‡è¿›è¡Œç»¼åˆç†è§£å’Œæ€»ç»“ã€‚ç³»ç»Ÿå°†åˆ†æç”¨æˆ·çš„ç¡çœ ç›‘æµ‹æ•°æ®å¹¶æä¾›ä¸“ä¸šè§£è¯»ã€‚é—®é¢˜å¯æ¶‰åŠç‰¹å®šæ—¥æœŸæˆ–æ—¶é—´æ®µçš„ç¡çœ è´¨é‡åˆ†æã€ç¡çœ è¶‹åŠ¿æ¯”è¾ƒã€ç¡çœ å¼‚å¸¸è§£é‡Šã€å¥åº·å»ºè®®ç­‰ã€‚ç³»ç»Ÿå°†æ ¹æ®é—®é¢˜è‡ªåŠ¨æ£€ç´¢ç›¸å…³çš„ç¡çœ æ•°æ®è®°å½•ï¼Œå¹¶ç»™å‡ºä¸“ä¸šçš„åˆ†æå’Œå»ºè®®ã€‚",
            "required": True
        }
        ]
    },
    {
        "name": "DirectLLMCommunityAiUser",
        "description": "é»˜è®¤å›ç­”å·¥å…·ï¼Œè´Ÿè´£æ— å·¥å…·è°ƒç”¨çš„å…œåº•ï¼Œä¸Šè¿°å·¥å…·æ— æ³•è°ƒç”¨æ—¶å¯è°ƒç”¨è¯¥å·¥å…·",
        "parameters": [
        {
            "name": "question",
            "description": "ç”¨æˆ·ä»»ä½•é—®é¢˜ï¼",
            "required": True
        }
        ]
    }
]


# æ„å»ºæ¶ˆæ¯
messages = [
    {"role": "user", "content": "æ–°å¢æ—¶è®¯æ¶ˆæ¯"},
    {"role": "assistant", "content": "Sure, I can help with that. Could you please provide me with the content for the new announcement?"},
    {"role": "user", "content": "æ˜å¤©ä¸­åˆ12ç‚¹å…¬å¸æ”¾å‡ï¼"}
]

# åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œè¿™é‡Œä¼šè‡ªåŠ¨æŒ‰ç…§åŸºåº§æ¨¡å‹qwenè¦æ±‚çš„æ ¼å¼å»è½¬æ¢è¾“å…¥å‚æ•°ï¼Œè½¬æ¢åè¾“å…¥æ¨¡å‹å‰çš„æœ€ç»ˆç»“æœå¯ä»¥æŸ¥çœ‹{decoded_input}ï¼Œä¸è®­ç»ƒæ•°æ®çš„è½¬æ¢ç»“æœä¸€è‡´
prompt = tokenizer.apply_chat_template(
    messages, 
    tools=tools,
    add_generation_prompt=True,
    tokenize=False
)

# ç”Ÿæˆå›å¤
inputs = tokenizer(prompt, return_tensors="pt")
input_length = inputs.input_ids.shape[1]
decoded_input = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)
outputs = model.generate(**inputs, max_new_tokens=256)
new_tokens = outputs[0][input_length:]  # æˆªå–æ–°ç”Ÿæˆçš„token
response = tokenizer.decode(new_tokens, skip_special_tokens=True)
print("===============================decode_input================================================")
print(f"{decoded_input}")
print("===============================decode_input================================================")
print("=================================response==============================================")
print(f"{response}")
print("=================================response==============================================")
```

<details>
æœ€ç»ˆè¾“å‡ºç»“æœ

===============================decode_input================================================
system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.
# Tools
You may call one or more functions to assist with the user query.
You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"name": "HandleTongzhiTonggao", "description": "è¯¥å·¥å…·ä¸“ç”¨äºæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤æ—¶è®¯æ¶ˆæ¯æˆ–é€šå‘Šï¼", "parameters": [{"name": "operation", "description": "ç”¨æˆ·å¯¹æ—¶è®¯æ¶ˆæ¯æˆ–é€šå‘Šçš„æ“ä½œæ–¹å¼ï¼Œä»åˆ—è¡¨ä¸­é€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼š['ADD', 'DELETE', 'UPDATE', 'LIST']", "required": true}, {"name": "type", "description": "ç”¨æˆ·ä¸Šä¼ æ¶ˆæ¯çš„ç±»å‹ï¼Œä»åˆ—è¡¨ä¸­é€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼š[\"æ—¶è®¯æ¶ˆæ¯\", \"é€šå‘Š\"]", "required": true}, {"name": "content", "description": "ç”¨æˆ·ä¸Šä¼ æ¶ˆæ¯çš„å†…å®¹", "required": true}]}
{"name": "WeatherApi", "description": "è¯¥å·¥å…·ä¸“ç”¨äºå¸‚ã€åŒºå¿çš„å®æ—¶å¤©æ°”æŸ¥è¯¢", "parameters": [{"name": "query_key", "description": "éœ€è¦æŸ¥è¯¢å¤©æ°”çš„åœ°åŒºï¼ˆå¯ä»¥æ˜¯ç›´è¾–å¸‚ã€åŒºå¿ï¼‰", "required": true}]}
{"name": "WaterMachineApi", "description": "ç‰©è”ç½‘é¥®æ°´æœºæ“ä½œæ¥å£ã€‚", "parameters": [{"name": "operation", "description": "ä»ç”¨æˆ·å…³äºé¥®æ°´æœºæ“ä½œçš„é—®é¢˜ä¸­æå–ç›¸å…³çš„æ“ä½œæŒ‡ä»¤ï¼Œå¯é€‰æ‹©çš„æŒ‡ä»¤åŒ…å«ï¼š[æ°´å£¶å–æ°´, æ°´å£¶åœæ°´, åœæ­¢åŠ çƒ­, å¼€å§‹ä¿æ¸©, åœæ­¢ä¿æ¸©, æ‰“å¼€è¯­éŸ³, å…³é—­è¯­éŸ³]", "required": true}]}
{"name": "HealthReportSimple", "description": "ä¸“é—¨ç”¨äºå¤„ç†ç”¨æˆ·ä¸ªäººå¥åº·ç›‘æµ‹æ•°æ®çš„æ‰€æœ‰æŸ¥è¯¢ã€‚å½“ç”¨æˆ·é—®é¢˜æ˜ç¡®æ¶‰åŠä»¥ä¸‹ä»»ä¸€æ–¹é¢æ—¶ï¼Œç³»ç»Ÿå¿…é¡»ä¼˜å…ˆè°ƒç”¨æ­¤å·¥å…·ï¼š(1)ç¡çœ ç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚\"ç¡çœ æŠ¥å‘Š\"ã€\"ç¡çœ æ•°æ®\"ã€\"ç¡çœ è´¨é‡\"ã€\"ç¡çœ æ—¶é•¿\"ã€\"ç¡çœ æ•ˆç‡\"ã€\"ç¡çœ è¯„åˆ†\"ã€\"æ·±åº¦ç¡çœ \"ã€\"æµ…åº¦ç¡çœ \"ï¼‰ï¼›(2)ç”Ÿç†æŒ‡æ ‡ï¼ˆå¦‚\"å¿ƒç‡\"ã€\"å¿ƒç‡å¼‚å¸¸\"ã€\"å‘¼å¸ç‡\"ã€\"å‘¼å¸å¼‚å¸¸\"ã€\"ä½“åŠ¨æ¬¡æ•°\"ã€\"ä½“åŠ¨æŒ‡æ•°\"ï¼‰ï¼›(3)å¥åº·çŠ¶æ€ï¼ˆå¦‚\"å¥åº·è¯„åˆ†\"ã€\"å¥åº·çŠ¶å†µ\"ã€\"å¥åº·å¼‚å¸¸\"ï¼‰ï¼›(4)ä»»ä½•æ¶‰åŠç”¨æˆ·ä¸ªäººç›‘æµ‹æ•°æ®çš„æ—¶é—´æ€§æŸ¥è¯¢ï¼ˆå¦‚\"æ˜¨æ™š\"ã€\"æœ€è¿‘å‡ å¤©\"ã€\"æœ¬å‘¨\"ã€\"ä¸Šå‘¨\"ç­‰æ—¶é—´æ®µçš„å¥åº·æ•°æ®ï¼‰ã€‚HealthReportæä¾›ä¸“ä¸šçš„å¥åº·æ•°æ®è§£è¯»å’Œåˆ†æã€‚", "parameters": [{"name": "health_report_question", "description": "ç”¨æˆ·å…³äºç¡çœ å¥åº·æŠ¥å‘Šçš„å®Œæ•´é—®é¢˜ï¼Œéœ€æ ¹æ®å½“å‰é—®é¢˜å’Œå†å²å¯¹è¯ä¸Šä¸‹æ–‡è¿›è¡Œç»¼åˆç†è§£å’Œæ€»ç»“ã€‚ç³»ç»Ÿå°†åˆ†æç”¨æˆ·çš„ç¡çœ ç›‘æµ‹æ•°æ®å¹¶æä¾›ä¸“ä¸šè§£è¯»ã€‚é—®é¢˜å¯æ¶‰åŠç‰¹å®šæ—¥æœŸæˆ–æ—¶é—´æ®µçš„ç¡çœ è´¨é‡åˆ†æã€ç¡çœ è¶‹åŠ¿æ¯”è¾ƒã€ç¡çœ å¼‚å¸¸è§£é‡Šã€å¥åº·å»ºè®®ç­‰ã€‚ç³»ç»Ÿå°†æ ¹æ®é—®é¢˜è‡ªåŠ¨æ£€ç´¢ç›¸å…³çš„ç¡çœ æ•°æ®è®°å½•ï¼Œå¹¶ç»™å‡ºä¸“ä¸šçš„åˆ†æå’Œå»ºè®®ã€‚", "required": true}]}
{"name": "DirectLLMCommunityAiUser", "description": "é»˜è®¤å›ç­”å·¥å…·ï¼Œè´Ÿè´£æ— å·¥å…·è°ƒç”¨çš„å…œåº•ï¼Œä¸Šè¿°å·¥å…·æ— æ³•è°ƒç”¨æ—¶å¯è°ƒç”¨è¯¥å·¥å…·", "parameters": [{"name": "question", "description": "ç”¨æˆ·ä»»ä½•é—®é¢˜ï¼", "required": true}]}
</tools>
For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>
user
æ–°å¢æ—¶è®¯æ¶ˆæ¯
assistant
Sure, I can help with that. Could you please provide me with the content for the new announcement?
user
æ˜å¤©ä¸­åˆ12ç‚¹å…¬å¸æ”¾å‡ï¼
assistant
===============================decode_input================================================

=================================response==============================================
<tool_call>
{"name": "HandleTongzhiTonggao", "arguments": {"operation": "ADD", "type": "æ—¶è®¯æ¶ˆæ¯", "content": "æ˜å¤©ä¸­åˆ12ç‚¹å…¬å¸æ”¾å‡ï¼"}}
</tool_call>
=================================response==============================================
</details>
```

---


## âš¡ æ¨¡å‹é‡åŒ–

### GGUF æ ¼å¼è½¬æ¢

å°† HuggingFace æ ¼å¼çš„æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼ï¼Œå¹¶è¿›è¡Œ 4 æ¯”ç‰¹é‡åŒ–ï¼š

```bash
# ğŸ“¦ ç¬¬ä¸€æ­¥ï¼šè½¬æ¢ä¸º F16 æ ¼å¼
python convert_hf_to_gguf.py \
    /root/autodl-tmp/merge_model_tool_call_20250816_qwen \
    --outfile /root/autodl-tmp/tool_call_f16.gguf \
    --outtype f16

# ğŸ—œï¸ ç¬¬äºŒæ­¥ï¼šé‡åŒ–ä¸º 4 æ¯”ç‰¹
./build/llama-quantize \
    /root/autodl-tmp/tool_call_f16.gguf \
    /root/autodl-tmp/tool_call_q4.gguf \
    /root/autodl-tmp/tool_call_q4.gguf \
    Q4_K_M
```

### é‡åŒ–æ ¼å¼å¯¹æ¯”

| æ ¼å¼ | æ–‡ä»¶å¤§å° | è´¨é‡ | æ¨èåœºæ™¯ |
|------|----------|------|----------|
| **F16** | ~13GB | æœ€é«˜ | å¼€å‘æµ‹è¯• |
| **Q8_0** | ~7GB | å¾ˆé«˜ | é«˜è´¨é‡æ¨ç† |
| **Q4_K_M** | ~4GB | è‰¯å¥½ | ç”Ÿäº§éƒ¨ç½² â­ |
| **Q4_0** | ~3.9GB | ä¸€èˆ¬ | èµ„æºå—é™ |

---

## ğŸ‹ Ollama éƒ¨ç½²

### Modelfile é…ç½®

åˆ›å»º `Modelfile` æ–‡ä»¶æ¥å®šä¹‰æ¨¡å‹é…ç½®ï¼š

```dockerfile
FROM /work/ai/model/tool_call_q4.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}{{ end }}{{ if .Tools }}
# Tools
You may call one or more functions to assist with the user query.
You are provided with function signatures within <tools></tools> XML tags:
<tools>
{{ range .Tools }}{{ . }}{{ end }}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>{{ end }}<|im_end|>
{{ if .Messages }}{{ range .Messages }}<|im_start|>{{ .Role }}
{{ .Content }}<|im_end|>
{{ end }}{{ end }}<|im_start|>assistant
"""

PARAMETER temperature 0.1
PARAMETER top_p 0.9
PARAMETER stop <|im_end|>
PARAMETER stop <|endoftext|>
```

### API è°ƒç”¨ç¤ºä¾‹

ä½¿ç”¨æ ‡å‡† OpenAI æ ¼å¼è¿›è¡Œ Function Callingï¼š

```bash
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "toolcall_qwen2.5_q4",
    "messages": [
      {
        "role": "user",
        "content": "æ–°å¢é€šå‘Š"
      },
      {
        "role": "assistant", 
        "content": "å¥½çš„ï¼Œæ‚¨æƒ³æ–°å¢ä»€ä¹ˆæ ·çš„é€šå‘Šå‘¢ï¼Ÿè¯·å‘Šè¯‰æˆ‘é€šå‘Šçš„å…·ä½“å†…å®¹ã€‚"
      },
      {
        "role": "user",
        "content": "ä»Šå¤©ä¸­åˆ12ç‚¹å…¬å¸æ”¾å‡"
      }
    ],
    "stream": false,
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "HandleTongzhiTonggao",
          "description": "è¯¥å·¥å…·ä¸“ç”¨äºæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤æ—¶è®¯æ¶ˆæ¯æˆ–é€šå‘Šï¼",
          "parameters": {
            "type": "object",
            "properties": {
              "operation": {
                "type": "string",
                "description": "ç”¨æˆ·å¯¹æ—¶è®¯æ¶ˆæ¯æˆ–é€šå‘Šçš„æ“ä½œæ–¹å¼",
                "enum": ["ADD", "DELETE", "UPDATE", "LIST"]
              },
              "type": {
                "type": "string", 
                "description": "ç”¨æˆ·ä¸Šä¼ æ¶ˆæ¯çš„ç±»å‹",
                "enum": ["æ—¶è®¯æ¶ˆæ¯", "é€šå‘Š"]
              },
              "content": {
                "type": "string",
                "description": "ç”¨æˆ·ä¸Šä¼ æ¶ˆæ¯çš„å†…å®¹"
              }
            },
            "required": ["operation", "type", "content"]
          }
        }
      }
    ]
  }'
```

### é¢„æœŸè¿”å›ç»“æœ

```json
{
  "model": "toolcall_qwen2.5_q4",
  "created_at": "2025-08-18T07:28:39.03524207Z",
  "message": {
    "role": "assistant",
    "content": "<tool_call>\n{\"name\": \"HandleTongzhiTonggao\", \"arguments\": {\"operation\": \"ADD\", \"type\": \"é€šå‘Š\", \"content\": \"ä»Šå¤©ä¸­åˆ12ç‚¹å…¬å¸æ”¾å‡\"}}\n</tool_call>"
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 950943370,
  "load_duration": 15252446,
  "prompt_eval_count": 242,
  "prompt_eval_duration": 17372578,
  "eval_count": 45,
  "eval_duration": 908369040
}
```

---

# ğŸš€ vLLM é‡åŒ–æ¨¡å‹éƒ¨ç½²

---

## ğŸ“‹ é‡åŒ–æ ¼å¼æ”¯æŒ

### vLLM æ”¯æŒçš„é‡åŒ–æ ¼å¼

| é‡åŒ–æ–¹æ³• | æ ¼å¼ | vLLM æ”¯æŒ | æ¨èåº¦ |
|---------|------|-----------|--------|
| **AWQ** | `.safetensors` | âœ… å®Œå…¨æ”¯æŒ | â­â­â­â­â­ |
| **GPTQ** | `.safetensors` | âœ… å®Œå…¨æ”¯æŒ | â­â­â­â­ |
| **SqueezeLLM** | `.safetensors` | âœ… æ”¯æŒ | â­â­â­ |
| **FP8** | `.safetensors` | âœ… æ”¯æŒ | â­â­â­â­ |
| **GGUF** | `.gguf` | âŒ ä¸æ”¯æŒ | âŒ |

> **é‡è¦æé†’ï¼š** vLLM **ä¸æ”¯æŒ** GGUF æ ¼å¼ï¼éœ€è¦ä½¿ç”¨ HuggingFace å…¼å®¹çš„é‡åŒ–æ ¼å¼ã€‚

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [LlamaFactory å®˜æ–¹æ–‡æ¡£](https://github.com/hiyouga/LLaMA-Factory)
- [llama.cpp æ„å»ºæ–‡æ¡£](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)
- [Ollama æ¨¡å‹æ–‡ä»¶æ ¼å¼](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

---

*ğŸ“… æœ€åæ›´æ–°ï¼š2025-08-18*